{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from torch.distributions import constraints\n",
    "from torch import nn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import Predictive\n",
    "import seaborn as sns\n",
    "from pyro import poutine\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PMF(nn.Module):\n",
    "    # by default our latent space is 50-dimensional\n",
    "    # and we use 400 hidden units\n",
    "    def __init__(self, train, dim):\n",
    "        super().__init__()\n",
    "        \"\"\"Build the Probabilistic Matrix Factorization model using pymc3.\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        self.dim = dim   \n",
    "        self.data = train.copy()\n",
    "        self.n, self.m = self.data.shape\n",
    "        self.map = None\n",
    "        self.bounds = (0,1)\n",
    "        self.losses = None\n",
    "        self.predictions = None\n",
    "        self.returned = None\n",
    "\n",
    "\n",
    "        # Perform mean value imputation\n",
    "    \n",
    "        \n",
    "        # Low precision reflects uncertainty; prevents overfitting.\n",
    "        # Set to the mean variance across users and items.\n",
    "        self.alpha_u = (np.mean(self.data, axis=1).mean())**2 / np.std(self.data, axis=1).mean()\n",
    "        self.alpha_v = (np.mean(self.data, axis=0).mean())**2 / np.std(self.data, axis=0).mean()\n",
    "        \n",
    "        self.beta_u = (np.mean(self.data, axis=1).mean()) / np.std(self.data, axis=1).mean()\n",
    "        self.beta_v = (np.mean(self.data, axis=0).mean()) / np.std(self.data, axis=0).mean()\n",
    "        self.bias = self.data.mean()\n",
    "\n",
    "\n",
    "    def model(self, train, mask):\n",
    "        a = 50\n",
    "\n",
    "        drug_plate = pyro.plate(\"drug_latents\", self.n, dim= -1) #independent users\n",
    "        sideeffect_plate = pyro.plate(\"sideeffect_latents\", self.m, dim= -1) #independent items\n",
    "\n",
    "        with drug_plate: \n",
    "            UA = pyro.sample(\"UA\", dist.Gamma(self.alpha_u, self.beta_u).expand([self.dim]).to_event(1))\n",
    "            #UA_int = pyro.sample(\"UAint\", dist.Normal(0., 1.))\n",
    "        \n",
    "        with sideeffect_plate:\n",
    "            VA = pyro.sample(\"VA\", dist.Gamma(self.alpha_v, self.beta_v).expand([self.dim]).to_event(1))\n",
    "            #possibly add intercepts VA_int = pyro.sample(\"VA\", dist.Normal(0., 1.).to_event(1))\n",
    "       \n",
    "        u2_plate = pyro.plate(\"u2_plate\", self.n, dim=-2)\n",
    "\n",
    "        with sideeffect_plate, u2_plate: \n",
    "            with pyro.poutine.mask(mask=mask):\n",
    "             Y = pyro.sample(\"target\", dist.NegativeBinomial(a, UA@VA.T/( UA@VA.T+a) ), obs=train ) \n",
    "             return Y\n",
    "        \n",
    "\n",
    "    def guide(self, train=None, mask=None):\n",
    "\n",
    "        d_alpha = pyro.param('d_alpha', torch.ones(self.n,self.dim), constraint=constraints.positive)#*self.user_mean)\n",
    "        d_beta = pyro.param('d_beta', 0.5*torch.ones(self.n,self.dim), constraint=constraints.positive)\n",
    "       # int_mean = pyro.param('int_mean', torch.tensor(1.)*self.user_mean)\n",
    "       # mov_cov = pyro.param('mov_cov', torch.tensor(1.)*0.1,\n",
    "          #                  constraint=constraints.positive)\n",
    "\n",
    "        s_alpha = pyro.param('s_alpha', torch.ones(self.m,self.dim), constraint=constraints.positive)#*self.item_mean)\n",
    "        s_beta = pyro.param('s_beta', 0.5*torch.ones(self.m,self.dim), constraint=constraints.positive)\n",
    "        drug_plate = pyro.plate(\"drug_latents\", self.n, dim= -1) #independent users\n",
    "        sideeffect_plate = pyro.plate(\"sideeffect_latents\", self.m, dim= -1) #independent items\n",
    "\n",
    "        with drug_plate: \n",
    "            UA = pyro.sample(\"UA\", dist.Gamma(d_alpha, d_beta).to_event(1))\n",
    "           # UA_int = pyro.sample(\"UAint\", dist.Normal(int_mean, mov_cov).to_event(1))\n",
    "        with sideeffect_plate: \n",
    "            VA = pyro.sample(\"VA\", dist.Gamma(s_alpha, s_beta).to_event(1))\n",
    "    \n",
    "    def train_SVI(self,train,mask, nsteps=250, lr = 0.05, lrd = 1):\n",
    "        logging.basicConfig(format='%(message)s', level=logging.INFO)\n",
    "        svi = SVI(self.model,\n",
    "        self.guide,\n",
    "        optim.ClippedAdam({\"lr\": lr, \"lrd\": lrd}),\n",
    "        loss=Trace_ELBO())\n",
    "        losses = []\n",
    "        for step in range(nsteps):\n",
    "            elbo = svi.step(torch.from_numpy(train).float(), mask)\n",
    "            losses.append(elbo)\n",
    "            if step % 10 == 0:\n",
    "                print(\"Elbo loss: {}\".format(elbo))\n",
    "        self.losses = losses\n",
    "        #constrained_params = list(pyro.get_param_store().values())\n",
    "        #PARAMS = [p.unconstrained() for p in constrained_params]\n",
    "        #print(PARAMS)\n",
    "        return losses\n",
    "    \n",
    "    def sample_predict(self, nsamples=500 , verbose=True):\n",
    "        unmasked =torch.ones((self.n,self.m), dtype=torch.bool)\n",
    "        predictive_svi = Predictive(self.model, guide=self.guide, num_samples=nsamples)(None , unmasked)\n",
    "        if (verbose):\n",
    "            for k, v in predictive_svi.items():\n",
    "                print(f\"{k}: {tuple(v.shape)}\")\n",
    "        table = predictive_svi[\"target\"].numpy()\n",
    "        print(table)\n",
    "        self.returned = table\n",
    "        mc_table = table.mean(axis = 0)\n",
    "        mc_table_std = table.std(axis = 0)\n",
    "        mc_table[mc_table < self.bounds[1]] = self.bounds[0]\n",
    "        mc_table[mc_table >= self.bounds[1]] = self.bounds[1]\n",
    "        self.predictions = mc_table\n",
    "        \n",
    "    \n",
    "    def rmse(self,test):\n",
    "        low, high = self.bounds\n",
    "        test_data = test.copy()\n",
    "        test_data[test_data < high] = low\n",
    "        test_data[test_data >= high] = high\n",
    "        sqerror = abs(test_data - self.predictions) ** 2  # squared error array\n",
    "        mse = sqerror.sum()/(test_data.shape[0]*test_data.shape[1])\n",
    "        print(\"PMF MAP training RMSE: %.5f\" % np.sqrt(mse))\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(test_data.astype(int).flatten(),  self.predictions.astype(int).flatten(), pos_label=1)\n",
    "        metrics.auc(fpr, tpr)\n",
    "        print(\"AUC: %.5f\" % metrics.auc(fpr, tpr))\n",
    "        return np.sqrt(mse) , metrics.auc(fpr, tpr)\n",
    "\n",
    "    def get_predictions(self):\n",
    "        return (self.returned,self.predictions)\n",
    "\n",
    "    \n",
    "   \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1127, 5237)\n",
      "tensor([[False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('data_all.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "print(data.shape)\n",
    "\n",
    "nan_mask = np.isnan(data) #when calculating the train/test set to \"nan\" all the examples that are for testing so that you do not train on them \n",
    "print(torch.from_numpy(nan_mask) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elbo loss: 9055118.078125\n",
      "Elbo loss: 9098882.40625\n",
      "Elbo loss: 9098319.78125\n",
      "Elbo loss: 8944937.421875\n",
      "Elbo loss: 8960036.609375\n",
      "Elbo loss: 8975340.296875\n",
      "Elbo loss: 8975446.015625\n",
      "Elbo loss: 8960144.546875\n",
      "Elbo loss: 8974696.4921875\n",
      "Elbo loss: 8968002.8671875\n",
      "Elbo loss: 8966022.4765625\n",
      "Elbo loss: 8903937.1484375\n",
      "Elbo loss: 8957122.9453125\n",
      "Elbo loss: 8926410.2265625\n",
      "Elbo loss: 8925323.78125\n",
      "Elbo loss: 9011655.4296875\n",
      "Elbo loss: 8939463.015625\n",
      "Elbo loss: 8934858.796875\n",
      "Elbo loss: 8910493.421875\n",
      "Elbo loss: 8971726.765625\n",
      "Elbo loss: 8934409.6484375\n",
      "Elbo loss: 8939027.8984375\n",
      "Elbo loss: 8913324.953125\n",
      "Elbo loss: 8867438.7578125\n",
      "Elbo loss: 8900528.0390625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9055118.078125,\n",
       " 9083290.515625,\n",
       " 9042242.46875,\n",
       " 9069657.359375,\n",
       " 9041994.7109375,\n",
       " 9152475.5234375,\n",
       " 9091170.0546875,\n",
       " 9057834.6640625,\n",
       " 9054962.546875,\n",
       " 9086004.734375,\n",
       " 9098882.40625,\n",
       " 9074073.7109375,\n",
       " 9104094.953125,\n",
       " 9075988.625,\n",
       " 9122053.78125,\n",
       " 9036397.59375,\n",
       " 9031093.109375,\n",
       " 9022227.2265625,\n",
       " 9049068.015625,\n",
       " 8996837.90625,\n",
       " 9098319.78125,\n",
       " 9060713.2734375,\n",
       " 9057273.359375,\n",
       " 9011294.296875,\n",
       " 9001157.75,\n",
       " 9026720.7421875,\n",
       " 9007314.875,\n",
       " 9022993.0859375,\n",
       " 9015175.3515625,\n",
       " 9013876.984375,\n",
       " 8944937.421875,\n",
       " 9014812.75,\n",
       " 8955290.6953125,\n",
       " 9037072.84375,\n",
       " 8952280.265625,\n",
       " 8989081.21875,\n",
       " 9036661.109375,\n",
       " 9033199.5546875,\n",
       " 9005478.7421875,\n",
       " 9010961.1328125,\n",
       " 8960036.609375,\n",
       " 8925865.4921875,\n",
       " 8939705.34375,\n",
       " 8965799.1328125,\n",
       " 8971165.4140625,\n",
       " 9038159.875,\n",
       " 8981584.1640625,\n",
       " 9000537.8046875,\n",
       " 8974634.4140625,\n",
       " 8936924.3984375,\n",
       " 8975340.296875,\n",
       " 8986144.875,\n",
       " 9002850.53125,\n",
       " 8986332.9375,\n",
       " 8928115.234375,\n",
       " 8966553.3515625,\n",
       " 8958798.046875,\n",
       " 9011376.640625,\n",
       " 8944591.359375,\n",
       " 8972089.6015625,\n",
       " 8975446.015625,\n",
       " 9010541.59375,\n",
       " 8981922.5078125,\n",
       " 9028892.46875,\n",
       " 9023908.03125,\n",
       " 9008879.78125,\n",
       " 8987051.9375,\n",
       " 8961911.4609375,\n",
       " 8966441.5234375,\n",
       " 8986759.0078125,\n",
       " 8960144.546875,\n",
       " 8967889.6328125,\n",
       " 8947887.359375,\n",
       " 8899214.109375,\n",
       " 8924851.6171875,\n",
       " 8977926.8984375,\n",
       " 8960852.0625,\n",
       " 9016562.25,\n",
       " 8919710.703125,\n",
       " 9001830.5703125,\n",
       " 8974696.4921875,\n",
       " 8945111.140625,\n",
       " 9063095.796875,\n",
       " 8989195.140625,\n",
       " 8953804.8125,\n",
       " 8932457.8203125,\n",
       " 8959149.5390625,\n",
       " 8954712.703125,\n",
       " 8968868.40625,\n",
       " 8997955.1640625,\n",
       " 8968002.8671875,\n",
       " 8952237.8046875,\n",
       " 8948022.8984375,\n",
       " 9044238.765625,\n",
       " 8988614.15625,\n",
       " 8966339.171875,\n",
       " 8967846.0234375,\n",
       " 8953342.875,\n",
       " 8973029.890625,\n",
       " 8947942.53125,\n",
       " 8966022.4765625,\n",
       " 8951190.0546875,\n",
       " 8969099.078125,\n",
       " 8987479.65625,\n",
       " 8951754.21875,\n",
       " 8946350.015625,\n",
       " 8895124.5,\n",
       " 8976008.546875,\n",
       " 8968268.1640625,\n",
       " 9005385.5703125,\n",
       " 8903937.1484375,\n",
       " 8949571.4140625,\n",
       " 8997880.015625,\n",
       " 9006382.6875,\n",
       " 8986943.875,\n",
       " 8939048.4296875,\n",
       " 8920583.140625,\n",
       " 8913572.9375,\n",
       " 8953582.546875,\n",
       " 8961985.890625,\n",
       " 8957122.9453125,\n",
       " 8999585.6875,\n",
       " 8946919.90625,\n",
       " 8961261.234375,\n",
       " 8903930.9609375,\n",
       " 8954921.453125,\n",
       " 8966686.4375,\n",
       " 8940362.9375,\n",
       " 8918895.1015625,\n",
       " 8872424.1171875,\n",
       " 8926410.2265625,\n",
       " 8960096.7421875,\n",
       " 8869693.2578125,\n",
       " 8981975.9921875,\n",
       " 8964643.1171875,\n",
       " 8959518.4921875,\n",
       " 8933207.859375,\n",
       " 8937020.0859375,\n",
       " 8921666.9375,\n",
       " 8961140.78125,\n",
       " 8925323.78125,\n",
       " 8982461.453125,\n",
       " 8920776.1484375,\n",
       " 8896121.3125,\n",
       " 9001981.28125,\n",
       " 9043126.0,\n",
       " 8972755.1796875,\n",
       " 8948422.203125,\n",
       " 8910307.6796875,\n",
       " 8978098.28125,\n",
       " 9011655.4296875,\n",
       " 8935949.9609375,\n",
       " 8911313.4765625,\n",
       " 8943678.0546875,\n",
       " 8961878.7265625,\n",
       " 8936273.859375,\n",
       " 8964013.359375,\n",
       " 8904952.171875,\n",
       " 8931890.1796875,\n",
       " 8887816.40625,\n",
       " 8939463.015625,\n",
       " 8872791.9765625,\n",
       " 8908578.859375,\n",
       " 8913925.0234375,\n",
       " 8934455.3984375,\n",
       " 8919641.5625,\n",
       " 8938911.2265625,\n",
       " 8890685.5234375,\n",
       " 8965892.546875,\n",
       " 8868852.03125,\n",
       " 8934858.796875,\n",
       " 8934824.421875,\n",
       " 8970743.078125,\n",
       " 8885348.8828125,\n",
       " 8978262.9609375,\n",
       " 8891446.6484375,\n",
       " 8912529.09375,\n",
       " 8952304.2265625,\n",
       " 8948708.796875,\n",
       " 8902809.609375,\n",
       " 8910493.421875,\n",
       " 8872897.671875,\n",
       " 8932190.21875,\n",
       " 8914316.2890625,\n",
       " 8889173.1875,\n",
       " 8876430.0234375,\n",
       " 8950471.7578125,\n",
       " 8927561.140625,\n",
       " 8907930.671875,\n",
       " 8944738.703125,\n",
       " 8971726.765625,\n",
       " 8860636.3671875,\n",
       " 8899679.90625,\n",
       " 8962958.9921875,\n",
       " 8920743.734375,\n",
       " 8870383.2265625,\n",
       " 8932103.90625,\n",
       " 8966640.6171875,\n",
       " 8927245.7578125,\n",
       " 8960161.890625,\n",
       " 8934409.6484375,\n",
       " 8893685.4921875,\n",
       " 8859479.9765625,\n",
       " 8906334.984375,\n",
       " 8870889.09375,\n",
       " 8933785.0390625,\n",
       " 8894406.5,\n",
       " 8914393.28125,\n",
       " 8892568.4609375,\n",
       " 8942881.9296875,\n",
       " 8939027.8984375,\n",
       " 8957792.1015625,\n",
       " 8980545.6328125,\n",
       " 8932605.1875,\n",
       " 8865418.3125,\n",
       " 8844004.953125,\n",
       " 8920704.1015625,\n",
       " 8896878.8515625,\n",
       " 8912897.4375,\n",
       " 8891131.3984375,\n",
       " 8913324.953125,\n",
       " 8856748.5625,\n",
       " 8900982.640625,\n",
       " 8907780.1484375,\n",
       " 8862914.5234375,\n",
       " 8883921.3125,\n",
       " 8933955.3984375,\n",
       " 8929207.09375,\n",
       " 8881723.390625,\n",
       " 8837300.9296875,\n",
       " 8867438.7578125,\n",
       " 8922394.1875,\n",
       " 8925858.1953125,\n",
       " 8888639.609375,\n",
       " 8898053.3515625,\n",
       " 8945459.2578125,\n",
       " 8872551.953125,\n",
       " 8894172.8984375,\n",
       " 8925043.4296875,\n",
       " 8838642.5625,\n",
       " 8900528.0390625,\n",
       " 8962504.6328125,\n",
       " 8880968.4609375,\n",
       " 8908248.515625,\n",
       " 8892288.9609375,\n",
       " 8939345.984375,\n",
       " 8927234.7734375,\n",
       " 8872351.84375,\n",
       " 8874168.7109375,\n",
       " 8800838.3203125]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = PMF(train=data, dim=100)\n",
    "test.train_SVI(data, ~torch.from_numpy(nan_mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UA: (1000, 1, 1127, 100)\n",
      "VA: (1000, 1, 5237, 100)\n",
      "target: (1000, 1127, 5237)\n",
      "[[[  0.   0.   0. ...   1.   4.   0.]\n",
      "  [  0.   0.   0. ...   1.   0.   0.]\n",
      "  [  0.   4.   0. ...   1.   9.   0.]\n",
      "  ...\n",
      "  [ 12.   0.   1. ... 105.  18.   1.]\n",
      "  [  4.   0.   0. ...   9.   9.   0.]\n",
      "  [  0.   0.   0. ...   0.   0.   0.]]\n",
      "\n",
      " [[  2.   3.   0. ...   0.   1.   0.]\n",
      "  [  0.   0.   0. ...   0.   2.   0.]\n",
      "  [  0.   0.   0. ...   1.   5.   0.]\n",
      "  ...\n",
      "  [ 33.   1.   0. ...  66.   7.   1.]\n",
      "  [  0.   0.   0. ...   5.  16.   0.]\n",
      "  [  0.   0.   0. ...   0.   0.   0.]]\n",
      "\n",
      " [[  1.   0.   0. ...   1.   2.   0.]\n",
      "  [  0.   0.   0. ...   0.   0.   0.]\n",
      "  [  1.   0.   0. ...   1.   3.   0.]\n",
      "  ...\n",
      "  [ 15.   0.   0. ...  48.   6.   0.]\n",
      "  [  5.   0.   0. ...   4.   8.   0.]\n",
      "  [  0.   0.   0. ...   0.   1.   0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[  1.   0.   0. ...   1.   1.   1.]\n",
      "  [  0.   0.   0. ...   1.   0.   0.]\n",
      "  [  1.   0.   0. ...   3.   1.   0.]\n",
      "  ...\n",
      "  [ 24.   0.   0. ...  20.  20.   0.]\n",
      "  [  2.   0.   0. ...   6.   5.   0.]\n",
      "  [  0.   0.   0. ...   0.   0.   0.]]\n",
      "\n",
      " [[  1.   1.   0. ...   0.   0.   1.]\n",
      "  [  0.   0.   0. ...   0.   2.   0.]\n",
      "  [  2.   1.   0. ...   5.   2.   1.]\n",
      "  ...\n",
      "  [  5.   0.   0. ...  45.  20.   3.]\n",
      "  [  5.   0.   0. ...   3.  25.   0.]\n",
      "  [  0.   0.   0. ...   0.   1.   0.]]\n",
      "\n",
      " [[  0.   0.   0. ...   0.   1.   0.]\n",
      "  [  0.   0.   0. ...   2.   0.   0.]\n",
      "  [  1.   0.   0. ...   6.   2.   0.]\n",
      "  ...\n",
      "  [  2.   0.   0. ...  33.  23.   1.]\n",
      "  [  0.   0.   0. ...   4.  11.   1.]\n",
      "  [  0.   0.   0. ...   0.   0.   0.]]]\n"
     ]
    }
   ],
   "source": [
    "test.sample_predict(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMF MAP training RMSE: 0.32548\n",
      "AUC: 0.84278\n",
      "(array([[[  0.,   0.,   0., ...,   1.,   4.,   0.],\n",
      "        [  0.,   0.,   0., ...,   1.,   0.,   0.],\n",
      "        [  0.,   4.,   0., ...,   1.,   9.,   0.],\n",
      "        ...,\n",
      "        [ 12.,   0.,   1., ..., 105.,  18.,   1.],\n",
      "        [  4.,   0.,   0., ...,   9.,   9.,   0.],\n",
      "        [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
      "\n",
      "       [[  2.,   3.,   0., ...,   0.,   1.,   0.],\n",
      "        [  0.,   0.,   0., ...,   0.,   2.,   0.],\n",
      "        [  0.,   0.,   0., ...,   1.,   5.,   0.],\n",
      "        ...,\n",
      "        [ 33.,   1.,   0., ...,  66.,   7.,   1.],\n",
      "        [  0.,   0.,   0., ...,   5.,  16.,   0.],\n",
      "        [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
      "\n",
      "       [[  1.,   0.,   0., ...,   1.,   2.,   0.],\n",
      "        [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
      "        [  1.,   0.,   0., ...,   1.,   3.,   0.],\n",
      "        ...,\n",
      "        [ 15.,   0.,   0., ...,  48.,   6.,   0.],\n",
      "        [  5.,   0.,   0., ...,   4.,   8.,   0.],\n",
      "        [  0.,   0.,   0., ...,   0.,   1.,   0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[  1.,   0.,   0., ...,   1.,   1.,   1.],\n",
      "        [  0.,   0.,   0., ...,   1.,   0.,   0.],\n",
      "        [  1.,   0.,   0., ...,   3.,   1.,   0.],\n",
      "        ...,\n",
      "        [ 24.,   0.,   0., ...,  20.,  20.,   0.],\n",
      "        [  2.,   0.,   0., ...,   6.,   5.,   0.],\n",
      "        [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
      "\n",
      "       [[  1.,   1.,   0., ...,   0.,   0.,   1.],\n",
      "        [  0.,   0.,   0., ...,   0.,   2.,   0.],\n",
      "        [  2.,   1.,   0., ...,   5.,   2.,   1.],\n",
      "        ...,\n",
      "        [  5.,   0.,   0., ...,  45.,  20.,   3.],\n",
      "        [  5.,   0.,   0., ...,   3.,  25.,   0.],\n",
      "        [  0.,   0.,   0., ...,   0.,   1.,   0.]],\n",
      "\n",
      "       [[  0.,   0.,   0., ...,   0.,   1.,   0.],\n",
      "        [  0.,   0.,   0., ...,   2.,   0.,   0.],\n",
      "        [  1.,   0.,   0., ...,   6.,   2.,   0.],\n",
      "        ...,\n",
      "        [  2.,   0.,   0., ...,  33.,  23.,   1.],\n",
      "        [  0.,   0.,   0., ...,   4.,  11.,   1.],\n",
      "        [  0.,   0.,   0., ...,   0.,   0.,   0.]]], dtype=float32), array([[1., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 1., 1., 0.],\n",
      "       ...,\n",
      "       [1., 0., 0., ..., 1., 1., 0.],\n",
      "       [1., 0., 0., ..., 1., 1., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))\n",
      "[[ 1  0  0 ...  1  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 1  0  0 ...  1  8  0]\n",
      " ...\n",
      " [ 8  0  0 ... 10 12  0]\n",
      " [ 1  0  0 ...  4 25  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "test.rmse(data)\n",
    "print(test.get_predictions())\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PMF_zero_inflated_poisson(nn.Module):\n",
    "\n",
    "    # by default our latent space is 50-dimensional\n",
    "    # and we use 400 hidden units\n",
    "    def __init__(self, train, dim):\n",
    "        super().__init__()\n",
    "        \"\"\"Build the Probabilistic Matrix Factorization model using pymc3.\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        self.dim = dim   \n",
    "        self.data = train.copy()\n",
    "        self.n, self.m = self.data.shape\n",
    "        self.map = None\n",
    "        self.bounds = (0,1)\n",
    "        self.losses = None\n",
    "        self.predictions = None\n",
    "        self.returned = None\n",
    "\n",
    "\n",
    "        # Perform mean value imputation\n",
    "    \n",
    "        \n",
    "        # Low precision reflects uncertainty; prevents overfitting.\n",
    "        # Set to the mean variance across users and items.\n",
    "        self.alpha_u = (np.mean(self.data, axis=1).mean())**2 / np.std(self.data, axis=1).mean()\n",
    "        self.alpha_v = (np.mean(self.data, axis=0).mean())**2 / np.std(self.data, axis=0).mean()\n",
    "        \n",
    "        self.beta_u = (np.mean(self.data, axis=1).mean()) / np.std(self.data, axis=1).mean()\n",
    "        self.beta_v = (np.mean(self.data, axis=0).mean()) / np.std(self.data, axis=0).mean()\n",
    "        self.bias = self.data.mean()\n",
    "        self.alpha = 1\n",
    "\n",
    "\n",
    "    def model(self, train, mask):\n",
    "        alpha = 1\n",
    "        beta = 1\n",
    "\n",
    "        drug_plate = pyro.plate(\"drug_latents\", self.n, dim= -1) #independent users\n",
    "        sideeffect_plate = pyro.plate(\"sideeffect_latents\", self.m, dim= -1) #independent items\n",
    "\n",
    "        with drug_plate: \n",
    "            UA = pyro.sample(\"UA\", dist.Gamma(self.alpha_u, self.beta_u).expand([self.dim]).to_event(1))\n",
    "            #alpha = pyro.sample(\"alpha\", dist.Poisson(self.alpha))\n",
    "            p = pyro.sample(\"p\", dist.Beta(alpha, beta))\n",
    "        \n",
    "        with sideeffect_plate:\n",
    "            VA = pyro.sample(\"VA\", dist.Gamma(self.alpha_v, self.beta_v).expand([self.dim]).to_event(1))\n",
    "            #possibly add intercepts VA_int = pyro.sample(\"VA\", dist.Normal(0., 1.).to_event(1))\n",
    "       \n",
    "        u2_plate = pyro.plate(\"u2_plate\", self.n, dim=-2)\n",
    "\n",
    "        with sideeffect_plate, u2_plate: \n",
    "           # with pyro.poutine.mask(mask=mask):\n",
    "             Y = pyro.sample(\"target\", dist.ZeroInflatedPoisson( rate = UA@VA.T ,gate = p[:, np.newaxis]), obs=train ) \n",
    "             return Y\n",
    "        \n",
    "\n",
    "    def guide(self, train=None, mask=None):\n",
    "\n",
    "        d_alpha = pyro.param('d_alpha', torch.ones(self.n,self.dim), constraint=constraints.positive)#*self.user_mean)\n",
    "        d_beta = pyro.param('d_beta', 0.5*torch.ones(self.n,self.dim), constraint=constraints.positive)\n",
    "        rate_alpha = pyro.param('rate_alpha', torch.ones(self.n), constraint=constraints.positive)\n",
    "        rate_beta = pyro.param('rate_beta', torch.ones(self.n), constraint=constraints.positive)\n",
    "\n",
    "\n",
    "        s_alpha = pyro.param('s_alpha', torch.ones(self.m,self.dim), constraint=constraints.positive)#*self.item_mean)\n",
    "        s_beta = pyro.param('s_beta', 0.5*torch.ones(self.m,self.dim), constraint=constraints.positive)\n",
    "        drug_plate = pyro.plate(\"drug_latents\", self.n, dim= -1) #independent users\n",
    "        sideeffect_plate = pyro.plate(\"sideeffect_latents\", self.m, dim= -1) #independent items\n",
    "\n",
    "        with drug_plate: \n",
    "            UA = pyro.sample(\"UA\", dist.Gamma(d_alpha, d_beta).to_event(1))\n",
    "            p = pyro.sample(\"p\", dist.Beta(rate_beta,rate_alpha))\n",
    "\n",
    "        with sideeffect_plate: \n",
    "            VA = pyro.sample(\"VA\", dist.Gamma(s_alpha, s_beta).to_event(1))\n",
    "    \n",
    "    def train_SVI(self,train,mask, nsteps=250, lr = 0.05, lrd = 1):\n",
    "        logging.basicConfig(format='%(message)s', level=logging.INFO)\n",
    "        svi = SVI(self.model,\n",
    "        self.guide,\n",
    "        optim.ClippedAdam({\"lr\": lr, \"lrd\": lrd}),\n",
    "        loss=Trace_ELBO())\n",
    "        losses = []\n",
    "        for step in range(nsteps):\n",
    "            elbo = svi.step(torch.from_numpy(train).float(), mask)\n",
    "            losses.append(elbo)\n",
    "            if step % 10 == 0:\n",
    "                print(\"Elbo loss: {}\".format(elbo))\n",
    "        self.losses = losses\n",
    "        #constrained_params = list(pyro.get_param_store().values())\n",
    "        #PARAMS = [p.unconstrained() for p in constrained_params]\n",
    "        #print(PARAMS)\n",
    "        return losses\n",
    "    \n",
    "    def sample_predict(self, nsamples=500 , verbose=True):\n",
    "        unmasked =torch.ones((self.n,self.m), dtype=torch.bool)\n",
    "        predictive_svi = Predictive(self.model, guide=self.guide, num_samples=nsamples)(None , unmasked)\n",
    "        if (verbose):\n",
    "            for k, v in predictive_svi.items():\n",
    "                print(f\"{k}: {tuple(v.shape)}\")\n",
    "        table = predictive_svi[\"target\"].numpy()\n",
    "        print(table)\n",
    "        self.returned = table\n",
    "        mc_table = table.mean(axis = 0)\n",
    "        mc_table_std = table.std(axis = 0)\n",
    "        mc_table[mc_table < self.bounds[1]] = self.bounds[0]\n",
    "        mc_table[mc_table >= self.bounds[1]] = self.bounds[1]\n",
    "        self.predictions = mc_table\n",
    "        \n",
    "    \n",
    "    def rmse(self,test):\n",
    "        low, high = self.bounds\n",
    "        test_data = test.copy()\n",
    "        test_data[test_data < high] = low\n",
    "        test_data[test_data >= high] = high\n",
    "        sqerror = abs(test_data - self.predictions) ** 2  # squared error array\n",
    "        mse = sqerror.sum()/(test_data.shape[0]*test_data.shape[1])\n",
    "        print(\"PMF MAP training RMSE: %.5f\" % np.sqrt(mse))\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(test_data.astype(int).flatten(),  self.predictions.astype(int).flatten(), pos_label=1)\n",
    "        metrics.auc(fpr, tpr)\n",
    "        print(\"AUC: %.5f\" % metrics.auc(fpr, tpr))\n",
    "        return np.sqrt(mse) , metrics.auc(fpr, tpr)\n",
    "\n",
    "    def get_predictions(self):\n",
    "        return (self.returned,self.predictions)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False]])\n",
      "Elbo loss: 13729418.012771606\n",
      "Elbo loss: 13750145.610977173\n",
      "Elbo loss: 13962415.76576233\n",
      "Elbo loss: 14119793.30834961\n",
      "Elbo loss: 13824654.43333435\n",
      "Elbo loss: 13529111.021270752\n",
      "Elbo loss: 13652855.801116943\n",
      "Elbo loss: 13721328.9821167\n",
      "Elbo loss: 13713437.802825928\n",
      "Elbo loss: 13583459.632843018\n",
      "Elbo loss: 13578454.516143799\n",
      "Elbo loss: 13874178.841888428\n",
      "Elbo loss: 13529643.261825562\n",
      "Elbo loss: 13496243.551498413\n",
      "Elbo loss: 13694373.06930542\n",
      "Elbo loss: 14108100.281860352\n",
      "Elbo loss: 13625978.810073853\n",
      "Elbo loss: 13809180.250991821\n",
      "Elbo loss: 13647418.549407959\n",
      "Elbo loss: 13414881.601287842\n",
      "Elbo loss: 13374836.144683838\n",
      "Elbo loss: 13600664.84361267\n",
      "Elbo loss: 13400532.42930603\n",
      "Elbo loss: 13844526.649917603\n",
      "Elbo loss: 13689517.901565552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[13729418.012771606,\n",
       " 13799094.6040802,\n",
       " 13937064.166641235,\n",
       " 14044982.368469238,\n",
       " 13740619.588027954,\n",
       " 14102335.373184204,\n",
       " 14144757.285995483,\n",
       " 14303679.13168335,\n",
       " 13957817.211029053,\n",
       " 14330843.800842285,\n",
       " 13750145.610977173,\n",
       " 14096368.707519531,\n",
       " 13901555.47517395,\n",
       " 14066655.321548462,\n",
       " 14226695.234725952,\n",
       " 13725984.370407104,\n",
       " 14066136.482330322,\n",
       " 13839212.206588745,\n",
       " 13854251.875823975,\n",
       " 13762393.408065796,\n",
       " 13962415.76576233,\n",
       " 14064696.379089355,\n",
       " 13823601.69203186,\n",
       " 13776417.682678223,\n",
       " 13716088.710235596,\n",
       " 13696205.110061646,\n",
       " 13693988.414520264,\n",
       " 13681650.319534302,\n",
       " 13648156.417098999,\n",
       " 13508108.145874023,\n",
       " 14119793.30834961,\n",
       " 13991413.649398804,\n",
       " 13794762.69897461,\n",
       " 13966015.728988647,\n",
       " 13651513.499816895,\n",
       " 14044038.64138794,\n",
       " 13633606.217758179,\n",
       " 13745714.428619385,\n",
       " 14126495.49293518,\n",
       " 14070257.738983154,\n",
       " 13824654.43333435,\n",
       " 13906832.114593506,\n",
       " 13913919.809814453,\n",
       " 13824604.572677612,\n",
       " 14103393.58883667,\n",
       " 13857544.905853271,\n",
       " 13991888.443603516,\n",
       " 13700721.345794678,\n",
       " 13793225.238800049,\n",
       " 13525100.225204468,\n",
       " 13529111.021270752,\n",
       " 13623236.04623413,\n",
       " 13847462.816070557,\n",
       " 13623978.28112793,\n",
       " 13680006.478683472,\n",
       " 13559410.892181396,\n",
       " 13737167.333129883,\n",
       " 13598294.677566528,\n",
       " 13431056.555389404,\n",
       " 13583312.256881714,\n",
       " 13652855.801116943,\n",
       " 13754870.826431274,\n",
       " 13894403.723098755,\n",
       " 13704693.23892212,\n",
       " 14243216.929626465,\n",
       " 13580083.903060913,\n",
       " 13998854.29750061,\n",
       " 13618909.575759888,\n",
       " 13643159.56552124,\n",
       " 13724620.031616211,\n",
       " 13721328.9821167,\n",
       " 13737178.832214355,\n",
       " 13636632.961257935,\n",
       " 13592303.459899902,\n",
       " 13628786.24130249,\n",
       " 13787359.337554932,\n",
       " 13519107.79953003,\n",
       " 14025359.223007202,\n",
       " 14014988.356292725,\n",
       " 13575294.263778687,\n",
       " 13713437.802825928,\n",
       " 13690423.216468811,\n",
       " 13597895.20928955,\n",
       " 13354111.753814697,\n",
       " 13898174.996963501,\n",
       " 13611293.487304688,\n",
       " 14066966.639221191,\n",
       " 13779971.907623291,\n",
       " 13413227.907608032,\n",
       " 13448140.626617432,\n",
       " 13583459.632843018,\n",
       " 13483678.874389648,\n",
       " 13722391.728652954,\n",
       " 13382150.498779297,\n",
       " 13996598.710571289,\n",
       " 13755467.281906128,\n",
       " 13850652.186279297,\n",
       " 13476744.79852295,\n",
       " 13462769.10482788,\n",
       " 13711182.94935608,\n",
       " 13578454.516143799,\n",
       " 13920373.706512451,\n",
       " 13875879.532180786,\n",
       " 13707157.538406372,\n",
       " 13698511.1796875,\n",
       " 13591791.520690918,\n",
       " 13899444.239807129,\n",
       " 13797231.930084229,\n",
       " 14082574.578338623,\n",
       " 13721235.628173828,\n",
       " 13874178.841888428,\n",
       " 13823634.45237732,\n",
       " 13799525.06564331,\n",
       " 13916041.459762573,\n",
       " 13747256.175613403,\n",
       " 13458239.838470459,\n",
       " 13736003.624450684,\n",
       " 13986653.620376587,\n",
       " 13513947.411239624,\n",
       " 13657121.526641846,\n",
       " 13529643.261825562,\n",
       " 13700439.79081726,\n",
       " 13954294.744003296,\n",
       " 13609472.732345581,\n",
       " 13939223.263259888,\n",
       " 13627482.794487,\n",
       " 13549329.063171387,\n",
       " 13623460.827919006,\n",
       " 13592679.202194214,\n",
       " 13836691.443939209,\n",
       " 13496243.551498413,\n",
       " 13666813.666687012,\n",
       " 13756109.817138672,\n",
       " 13919711.709243774,\n",
       " 13940361.817382812,\n",
       " 13473045.154434204,\n",
       " 13599435.095581055,\n",
       " 13784041.64187622,\n",
       " 13654306.625152588,\n",
       " 13607304.616790771,\n",
       " 13694373.06930542,\n",
       " 13877518.685592651,\n",
       " 13622657.510543823,\n",
       " 13783372.393173218,\n",
       " 13654865.122650146,\n",
       " 13284781.93510437,\n",
       " 13855441.467338562,\n",
       " 13572560.54977417,\n",
       " 13620378.083068848,\n",
       " 13816598.405670166,\n",
       " 14108100.281860352,\n",
       " 13556998.828475952,\n",
       " 13460751.376113892,\n",
       " 13677591.436599731,\n",
       " 13748131.21736145,\n",
       " 13913519.280151367,\n",
       " 13722049.856170654,\n",
       " 13787097.776641846,\n",
       " 13337036.198348999,\n",
       " 13768114.28414917,\n",
       " 13625978.810073853,\n",
       " 13943926.467910767,\n",
       " 13701403.931427002,\n",
       " 13653269.398757935,\n",
       " 13453749.915176392,\n",
       " 13510178.417953491,\n",
       " 13780023.31376648,\n",
       " 13445981.152160645,\n",
       " 13458894.027954102,\n",
       " 13325532.001586914,\n",
       " 13809180.250991821,\n",
       " 13423336.095916748,\n",
       " 13579563.300964355,\n",
       " 13696509.66532898,\n",
       " 13566879.718780518,\n",
       " 13397531.40560913,\n",
       " 13732226.652420044,\n",
       " 13728568.715621948,\n",
       " 13760106.383865356,\n",
       " 13968774.848937988,\n",
       " 13647418.549407959,\n",
       " 13222553.479553223,\n",
       " 13313057.532577515,\n",
       " 13834914.661193848,\n",
       " 13741282.903640747,\n",
       " 13491357.133865356,\n",
       " 13461724.745483398,\n",
       " 13375391.79460144,\n",
       " 13679206.253433228,\n",
       " 13468374.443695068,\n",
       " 13414881.601287842,\n",
       " 13400413.369766235,\n",
       " 13429203.673065186,\n",
       " 13436822.683448792,\n",
       " 13322227.133728027,\n",
       " 13598609.221633911,\n",
       " 13706395.301605225,\n",
       " 13483045.776870728,\n",
       " 13698571.545181274,\n",
       " 13298936.241790771,\n",
       " 13374836.144683838,\n",
       " 13506045.235351562,\n",
       " 13678767.032745361,\n",
       " 13660140.295318604,\n",
       " 13352235.955352783,\n",
       " 13613911.450942993,\n",
       " 13543243.744110107,\n",
       " 13945738.955413818,\n",
       " 13620636.242080688,\n",
       " 13487962.581756592,\n",
       " 13600664.84361267,\n",
       " 13708810.613586426,\n",
       " 13766530.82611084,\n",
       " 13624515.041015625,\n",
       " 13704473.329673767,\n",
       " 13797845.139602661,\n",
       " 13788514.435302734,\n",
       " 13605724.02772522,\n",
       " 13407350.798675537,\n",
       " 13854589.354660034,\n",
       " 13400532.42930603,\n",
       " 13298533.769317627,\n",
       " 13630428.904586792,\n",
       " 13919280.353302002,\n",
       " 13760868.953109741,\n",
       " 13578366.779769897,\n",
       " 13585050.56576538,\n",
       " 13484257.095809937,\n",
       " 13538856.76071167,\n",
       " 13928270.899353027,\n",
       " 13844526.649917603,\n",
       " 13832901.685958862,\n",
       " 13645212.723831177,\n",
       " 13348279.601760864,\n",
       " 13533801.770706177,\n",
       " 13535077.74168396,\n",
       " 13461996.464736938,\n",
       " 13496208.256347656,\n",
       " 13396904.181350708,\n",
       " 13793013.01437378,\n",
       " 13689517.901565552,\n",
       " 13778460.791519165,\n",
       " 13375319.671417236,\n",
       " 13600082.685012817,\n",
       " 13452810.718009949,\n",
       " 13374204.224243164,\n",
       " 13318523.851776123,\n",
       " 13647800.164100647,\n",
       " 13407624.740646362,\n",
       " 13629577.192718506]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nan_mask = np.isnan(data) #when calculating the train/test set to \"nan\" all the examples that are for testing so that you do not train on them \n",
    "print(torch.from_numpy(nan_mask) )\n",
    "test = PMF_zero_inflated_poisson(train=data, dim=100)\n",
    "test.train_SVI(data, ~torch.from_numpy(nan_mask))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UA: (1000, 1, 1127, 100)\n",
      "p: (1000, 1, 1127)\n",
      "VA: (1000, 1, 5237, 100)\n",
      "target: (1000, 1127, 5237)\n",
      "[[[ 0.  0.  0. ...  0.  0.  0.]\n",
      "  [ 0.  2.  0. ...  0.  0.  0.]\n",
      "  [ 2.  1.  0. ...  0.  0.  0.]\n",
      "  ...\n",
      "  [ 0.  2.  1. ...  0.  0.  1.]\n",
      "  [ 5.  0.  0. ...  0. 30.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...  0.  1.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]\n",
      "  [ 1.  0.  0. ...  0.  0.  0.]\n",
      "  ...\n",
      "  [16.  1.  0. ... 13. 17.  0.]\n",
      "  [ 2.  0.  0. ...  4. 20.  3.]\n",
      "  [ 0.  1.  0. ...  3.  1.  1.]]\n",
      "\n",
      " [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]\n",
      "  [ 1.  0.  0. ...  4.  0.  0.]\n",
      "  ...\n",
      "  [ 0.  0.  0. ...  6.  0.  0.]\n",
      "  [ 3.  0.  1. ...  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 3.  0.  0. ...  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]\n",
      "  [ 2.  0.  0. ...  0.  7.  1.]\n",
      "  ...\n",
      "  [12.  1.  2. ...  2.  0.  1.]\n",
      "  [ 3.  1.  0. ...  9. 29.  1.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  1.]\n",
      "  ...\n",
      "  [31.  0.  0. ... 15.  0.  0.]\n",
      "  [ 2.  0.  0. ...  0. 19.  0.]\n",
      "  [ 1.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...  0.  0.  1.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]\n",
      "  [ 3.  0.  0. ...  2. 11.  0.]\n",
      "  ...\n",
      "  [ 0.  0.  2. ... 10. 26.  0.]\n",
      "  [ 0.  1.  0. ...  0. 42.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "test.sample_predict(1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMF MAP training RMSE: 0.36273\n",
      "AUC: 0.81252\n",
      "(array([[[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 0.,  2.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 2.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  2.,  1., ...,  0.,  0.,  1.],\n",
      "        [ 5.,  0.,  0., ...,  0., 30.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
      "\n",
      "       [[ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        ...,\n",
      "        [16.,  1.,  0., ..., 13., 17.,  0.],\n",
      "        [ 2.,  0.,  0., ...,  4., 20.,  3.],\n",
      "        [ 0.,  1.,  0., ...,  3.,  1.,  1.]],\n",
      "\n",
      "       [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0., ...,  4.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  0., ...,  6.,  0.,  0.],\n",
      "        [ 3.,  0.,  1., ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 3.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 2.,  0.,  0., ...,  0.,  7.,  1.],\n",
      "        ...,\n",
      "        [12.,  1.,  2., ...,  2.,  0.,  1.],\n",
      "        [ 3.,  1.,  0., ...,  9., 29.,  1.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
      "\n",
      "       [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
      "        ...,\n",
      "        [31.,  0.,  0., ..., 15.,  0.,  0.],\n",
      "        [ 2.,  0.,  0., ...,  0., 19.,  0.],\n",
      "        [ 1.,  0.,  0., ...,  0.,  0.,  0.]],\n",
      "\n",
      "       [[ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 3.,  0.,  0., ...,  2., 11.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  2., ..., 10., 26.,  0.],\n",
      "        [ 0.,  1.,  0., ...,  0., 42.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 1., 1., 0.],\n",
      "       ...,\n",
      "       [1., 0., 0., ..., 1., 1., 0.],\n",
      "       [1., 0., 0., ..., 1., 1., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))\n",
      "[[ 1  0  0 ...  1  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 1  0  0 ...  1  8  0]\n",
      " ...\n",
      " [ 8  0  0 ... 10 12  0]\n",
      " [ 1  0  0 ...  4 25  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "test.rmse(data)\n",
    "print(test.get_predictions())\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PMF_zero_NB(nn.Module):\n",
    "\n",
    "    # by default our latent space is 50-dimensional\n",
    "    # and we use 400 hidden units\n",
    "    def __init__(self, train, dim):\n",
    "        super().__init__()\n",
    "        \"\"\"Build the Probabilistic Matrix Factorization model using pymc3.\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        self.dim = dim   \n",
    "        self.data = train.copy()\n",
    "        self.n, self.m = self.data.shape\n",
    "        self.map = None\n",
    "        self.bounds = (0,1)\n",
    "        self.losses = None\n",
    "        self.predictions = None\n",
    "        self.returned = None\n",
    "\n",
    "\n",
    "        # Perform mean value imputation\n",
    "    \n",
    "        \n",
    "        # Low precision reflects uncertainty; prevents overfitting.\n",
    "        # Set to the mean variance across users and items.\n",
    "        self.alpha_u = (np.mean(self.data, axis=1).mean())**2 / np.std(self.data, axis=1).mean()\n",
    "        self.alpha_v = (np.mean(self.data, axis=0).mean())**2 / np.std(self.data, axis=0).mean()\n",
    "        \n",
    "        self.beta_u = (np.mean(self.data, axis=1).mean()) / np.std(self.data, axis=1).mean()\n",
    "        self.beta_v = (np.mean(self.data, axis=0).mean()) / np.std(self.data, axis=0).mean()\n",
    "        self.bias = self.data.mean()\n",
    "        self.alpha = 1\n",
    "\n",
    "\n",
    "    def model(self, train, mask):\n",
    "        alpha = 1\n",
    "        beta = 1\n",
    "\n",
    "        drug_plate = pyro.plate(\"drug_latents\", self.n, dim= -1) #independent users\n",
    "        sideeffect_plate = pyro.plate(\"sideeffect_latents\", self.m, dim= -1) #independent items\n",
    "\n",
    "        with drug_plate: \n",
    "            UA = pyro.sample(\"UA\", dist.Gamma(self.alpha_u, self.beta_u).expand([self.dim]).to_event(1))\n",
    "            #alpha = pyro.sample(\"alpha\", dist.Poisson(self.alpha))\n",
    "            p = pyro.sample(\"p\", dist.Beta(alpha, beta))\n",
    "        \n",
    "        with sideeffect_plate:\n",
    "            VA = pyro.sample(\"VA\", dist.Gamma(self.alpha_v, self.beta_v).expand([self.dim]).to_event(1))\n",
    "            #possibly add intercepts VA_int = pyro.sample(\"VA\", dist.Normal(0., 1.).to_event(1))\n",
    "       \n",
    "        u2_plate = pyro.plate(\"u2_plate\", self.n, dim=-2)\n",
    "\n",
    "        with sideeffect_plate, u2_plate: \n",
    "           # with pyro.poutine.mask(mask=mask):\n",
    "             Y = pyro.sample(\"target\", dist.ZeroInflatedDistribution( base_dist= dist.NegativeBinomial(alpha, UA@VA.T/( UA@VA.T+alpha)) ,gate = p[:, np.newaxis]), obs=train ) \n",
    "             return Y\n",
    "        \n",
    "\n",
    "    def guide(self, train=None, mask=None):\n",
    "\n",
    "        d_alpha = pyro.param('d_alpha', torch.ones(self.n,self.dim), constraint=constraints.positive)#*self.user_mean)\n",
    "        d_beta = pyro.param('d_beta', 0.5*torch.ones(self.n,self.dim), constraint=constraints.positive)\n",
    "        rate_alpha = pyro.param('rate_alpha', torch.ones(self.n), constraint=constraints.positive)\n",
    "        rate_beta = pyro.param('rate_beta', torch.ones(self.n), constraint=constraints.positive)\n",
    "\n",
    "\n",
    "        s_alpha = pyro.param('s_alpha', torch.ones(self.m,self.dim), constraint=constraints.positive)#*self.item_mean)\n",
    "        s_beta = pyro.param('s_beta', 0.5*torch.ones(self.m,self.dim), constraint=constraints.positive)\n",
    "        drug_plate = pyro.plate(\"drug_latents\", self.n, dim= -1) #independent users\n",
    "        sideeffect_plate = pyro.plate(\"sideeffect_latents\", self.m, dim= -1) #independent items\n",
    "\n",
    "        with drug_plate: \n",
    "            UA = pyro.sample(\"UA\", dist.Gamma(d_alpha, d_beta).to_event(1))\n",
    "            p = pyro.sample(\"p\", dist.Beta(rate_beta,rate_alpha))\n",
    "\n",
    "        with sideeffect_plate: \n",
    "            VA = pyro.sample(\"VA\", dist.Gamma(s_alpha, s_beta).to_event(1))\n",
    "    \n",
    "    def train_SVI(self,train,mask, nsteps=250, lr = 0.05, lrd = 1):\n",
    "        logging.basicConfig(format='%(message)s', level=logging.INFO)\n",
    "        svi = SVI(self.model,\n",
    "        self.guide,\n",
    "        optim.ClippedAdam({\"lr\": lr, \"lrd\": lrd}),\n",
    "        loss=Trace_ELBO())\n",
    "        losses = []\n",
    "        for step in range(nsteps):\n",
    "            elbo = svi.step(torch.from_numpy(train).float(), mask)\n",
    "            losses.append(elbo)\n",
    "            if step % 10 == 0:\n",
    "                print(\"Elbo loss: {}\".format(elbo))\n",
    "        self.losses = losses\n",
    "        #constrained_params = list(pyro.get_param_store().values())\n",
    "        #PARAMS = [p.unconstrained() for p in constrained_params]\n",
    "        #print(PARAMS)\n",
    "        return losses\n",
    "    \n",
    "    def sample_predict(self, nsamples=500 , verbose=True):\n",
    "        unmasked =torch.ones((self.n,self.m), dtype=torch.bool)\n",
    "        predictive_svi = Predictive(self.model, guide=self.guide, num_samples=nsamples)(None , unmasked)\n",
    "        if (verbose):\n",
    "            for k, v in predictive_svi.items():\n",
    "                print(f\"{k}: {tuple(v.shape)}\")\n",
    "        table = predictive_svi[\"target\"].numpy()\n",
    "        print(table)\n",
    "        self.returned = table\n",
    "        mc_table = table.mean(axis = 0)\n",
    "        mc_table_std = table.std(axis = 0)\n",
    "        mc_table[mc_table < self.bounds[1]] = self.bounds[0]\n",
    "        mc_table[mc_table >= self.bounds[1]] = self.bounds[1]\n",
    "        self.predictions = mc_table\n",
    "        \n",
    "    \n",
    "    def rmse(self,test):\n",
    "        low, high = self.bounds\n",
    "        test_data = test.copy()\n",
    "        test_data[test_data < high] = low\n",
    "        test_data[test_data >= high] = high\n",
    "        sqerror = abs(test_data - self.predictions) ** 2  # squared error array\n",
    "        mse = sqerror.sum()/(test_data.shape[0]*test_data.shape[1])\n",
    "        print(\"PMF MAP training RMSE: %.5f\" % np.sqrt(mse))\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(test_data.astype(int).flatten(),  self.predictions.astype(int).flatten(), pos_label=1)\n",
    "        metrics.auc(fpr, tpr)\n",
    "        print(\"AUC: %.5f\" % metrics.auc(fpr, tpr))\n",
    "        return np.sqrt(mse) , metrics.auc(fpr, tpr)\n",
    "\n",
    "    def get_predictions(self):\n",
    "        return (self.returned,self.predictions)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False]])\n",
      "Elbo loss: 6012035.456176758\n",
      "Elbo loss: 6026213.563171387\n",
      "Elbo loss: 5999806.37689209\n",
      "Elbo loss: 6007662.76953125\n",
      "Elbo loss: 5995980.518127441\n",
      "Elbo loss: 5969965.423828125\n",
      "Elbo loss: 6014862.355224609\n",
      "Elbo loss: 5978878.61340332\n",
      "Elbo loss: 5958227.224975586\n",
      "Elbo loss: 5949028.366088867\n",
      "Elbo loss: 5967524.559936523\n",
      "Elbo loss: 5937743.723571777\n",
      "Elbo loss: 5964058.8966674805\n",
      "Elbo loss: 5960032.478393555\n",
      "Elbo loss: 5957602.663635254\n",
      "Elbo loss: 5935067.991943359\n",
      "Elbo loss: 5938172.410766602\n",
      "Elbo loss: 5915344.634277344\n",
      "Elbo loss: 5933078.581726074\n",
      "Elbo loss: 5929822.35534668\n",
      "Elbo loss: 5946331.634033203\n",
      "Elbo loss: 5940296.6384887695\n",
      "Elbo loss: 5917414.670532227\n",
      "Elbo loss: 5902229.934265137\n",
      "Elbo loss: 5920493.146972656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6012035.456176758,\n",
       " 6012342.945617676,\n",
       " 6004319.902587891,\n",
       " 6017493.768127441,\n",
       " 6034775.2158203125,\n",
       " 6007978.318664551,\n",
       " 6030311.8955078125,\n",
       " 5986345.342956543,\n",
       " 6015944.429016113,\n",
       " 6018410.323974609,\n",
       " 6026213.563171387,\n",
       " 6028207.652770996,\n",
       " 6013876.738891602,\n",
       " 6020635.438964844,\n",
       " 6039289.8212890625,\n",
       " 5998328.465454102,\n",
       " 6028851.170043945,\n",
       " 5993492.186157227,\n",
       " 6026789.848510742,\n",
       " 6003249.84185791,\n",
       " 5999806.37689209,\n",
       " 6022679.270263672,\n",
       " 6039606.224121094,\n",
       " 5974590.749328613,\n",
       " 5992812.397521973,\n",
       " 6003235.42980957,\n",
       " 6026491.765380859,\n",
       " 6026796.878845215,\n",
       " 6000504.59576416,\n",
       " 5976955.481811523,\n",
       " 6007662.76953125,\n",
       " 5970413.448547363,\n",
       " 5988363.527587891,\n",
       " 5985468.2060546875,\n",
       " 5962649.883850098,\n",
       " 6008944.888000488,\n",
       " 6005564.186889648,\n",
       " 5974745.263000488,\n",
       " 5984125.41796875,\n",
       " 6015425.143432617,\n",
       " 5995980.518127441,\n",
       " 5986274.057678223,\n",
       " 5970616.791931152,\n",
       " 5985542.175231934,\n",
       " 5971055.80456543,\n",
       " 5971207.098693848,\n",
       " 5976890.8572387695,\n",
       " 5988524.072509766,\n",
       " 6006659.8359375,\n",
       " 5955468.06842041,\n",
       " 5969965.423828125,\n",
       " 5986882.206359863,\n",
       " 5989162.155639648,\n",
       " 5991813.727905273,\n",
       " 5976948.073364258,\n",
       " 5970323.7443237305,\n",
       " 6000486.051086426,\n",
       " 5974122.28326416,\n",
       " 5959504.445922852,\n",
       " 5976980.264526367,\n",
       " 6014862.355224609,\n",
       " 5983132.149291992,\n",
       " 5981735.092529297,\n",
       " 6001747.267944336,\n",
       " 5983726.092041016,\n",
       " 5988810.618652344,\n",
       " 5985008.983581543,\n",
       " 5964766.074951172,\n",
       " 5954032.092407227,\n",
       " 5985302.364624023,\n",
       " 5978878.61340332,\n",
       " 5982831.3876953125,\n",
       " 5966951.679748535,\n",
       " 5950585.236206055,\n",
       " 5975395.490844727,\n",
       " 5973199.916931152,\n",
       " 5985470.321716309,\n",
       " 5950166.637390137,\n",
       " 5946843.063720703,\n",
       " 5980596.039550781,\n",
       " 5958227.224975586,\n",
       " 5968138.903442383,\n",
       " 5977810.573669434,\n",
       " 6002096.495178223,\n",
       " 5963673.753112793,\n",
       " 5947687.0830078125,\n",
       " 5954300.892944336,\n",
       " 5954239.937988281,\n",
       " 5943115.663330078,\n",
       " 5971635.339477539,\n",
       " 5949028.366088867,\n",
       " 5939549.659851074,\n",
       " 5956260.347900391,\n",
       " 5970557.2400512695,\n",
       " 5977413.0087890625,\n",
       " 5963070.16809082,\n",
       " 5937715.61517334,\n",
       " 5959804.203063965,\n",
       " 5965693.557006836,\n",
       " 5932419.540588379,\n",
       " 5967524.559936523,\n",
       " 5945860.823364258,\n",
       " 5972727.115234375,\n",
       " 5977215.467956543,\n",
       " 5957805.066650391,\n",
       " 5944966.321777344,\n",
       " 5975933.185058594,\n",
       " 5978522.262329102,\n",
       " 5973203.892028809,\n",
       " 5958242.575256348,\n",
       " 5937743.723571777,\n",
       " 5963552.245605469,\n",
       " 5959344.259277344,\n",
       " 5970134.016418457,\n",
       " 5933943.878845215,\n",
       " 5941202.937438965,\n",
       " 5956200.424499512,\n",
       " 5949595.398132324,\n",
       " 5959300.196166992,\n",
       " 5977785.846374512,\n",
       " 5964058.8966674805,\n",
       " 5974563.420959473,\n",
       " 5945769.282775879,\n",
       " 5966930.4853515625,\n",
       " 5963188.5546875,\n",
       " 5952430.010986328,\n",
       " 5960768.261474609,\n",
       " 5941176.988586426,\n",
       " 5948590.233947754,\n",
       " 5943135.304626465,\n",
       " 5960032.478393555,\n",
       " 5963478.901733398,\n",
       " 5930974.392272949,\n",
       " 5970839.510314941,\n",
       " 5953758.660522461,\n",
       " 5931193.178405762,\n",
       " 5942461.898010254,\n",
       " 5931196.600891113,\n",
       " 5941050.930908203,\n",
       " 5961827.117431641,\n",
       " 5957602.663635254,\n",
       " 5939638.539367676,\n",
       " 5936204.577636719,\n",
       " 5937309.114624023,\n",
       " 5960298.875244141,\n",
       " 5983788.036193848,\n",
       " 5957436.669189453,\n",
       " 5948570.860473633,\n",
       " 5942010.804382324,\n",
       " 5936743.244140625,\n",
       " 5935067.991943359,\n",
       " 5937582.55871582,\n",
       " 5973050.456604004,\n",
       " 5944254.4884643555,\n",
       " 5971313.098571777,\n",
       " 5940891.223815918,\n",
       " 5961814.2421875,\n",
       " 5954472.5759887695,\n",
       " 5922964.038452148,\n",
       " 5937431.847595215,\n",
       " 5938172.410766602,\n",
       " 5942969.379455566,\n",
       " 5961425.009643555,\n",
       " 5944660.200500488,\n",
       " 5929630.270874023,\n",
       " 5936278.456665039,\n",
       " 5964964.307800293,\n",
       " 5936076.92199707,\n",
       " 5947204.498413086,\n",
       " 5952898.177429199,\n",
       " 5915344.634277344,\n",
       " 5938644.155212402,\n",
       " 5921970.953491211,\n",
       " 5930049.273803711,\n",
       " 5941459.977783203,\n",
       " 5938466.39855957,\n",
       " 5926456.29498291,\n",
       " 5940774.200683594,\n",
       " 5912872.720825195,\n",
       " 5914624.127807617,\n",
       " 5933078.581726074,\n",
       " 5941981.849609375,\n",
       " 5958152.815734863,\n",
       " 5951266.643859863,\n",
       " 5926877.00592041,\n",
       " 5935830.084716797,\n",
       " 5939381.526977539,\n",
       " 5926788.493408203,\n",
       " 5935021.9716796875,\n",
       " 5930053.993041992,\n",
       " 5929822.35534668,\n",
       " 5900785.633666992,\n",
       " 5898995.497436523,\n",
       " 5915199.233825684,\n",
       " 5939202.110961914,\n",
       " 5932826.355041504,\n",
       " 5934678.107666016,\n",
       " 5923485.973510742,\n",
       " 5931131.012939453,\n",
       " 5916406.616027832,\n",
       " 5946331.634033203,\n",
       " 5943439.299621582,\n",
       " 5941981.8416137695,\n",
       " 5944375.107543945,\n",
       " 5929062.151733398,\n",
       " 5902421.925170898,\n",
       " 5925964.497253418,\n",
       " 5907158.252685547,\n",
       " 5944367.542358398,\n",
       " 5941806.574279785,\n",
       " 5940296.6384887695,\n",
       " 5932105.598815918,\n",
       " 5927649.120422363,\n",
       " 5937451.438354492,\n",
       " 5923051.003356934,\n",
       " 5926995.12689209,\n",
       " 5939304.232299805,\n",
       " 5904083.769836426,\n",
       " 5910709.937255859,\n",
       " 5934026.994445801,\n",
       " 5917414.670532227,\n",
       " 5908483.177368164,\n",
       " 5914735.543701172,\n",
       " 5924134.338012695,\n",
       " 5901321.4728393555,\n",
       " 5915232.135070801,\n",
       " 5933779.7052612305,\n",
       " 5922940.827453613,\n",
       " 5926298.061950684,\n",
       " 5916525.661987305,\n",
       " 5902229.934265137,\n",
       " 5912839.914916992,\n",
       " 5920708.6591796875,\n",
       " 5918290.356567383,\n",
       " 5915142.637756348,\n",
       " 5914371.724609375,\n",
       " 5901327.310791016,\n",
       " 5910603.616760254,\n",
       " 5928094.186767578,\n",
       " 5899462.851135254,\n",
       " 5920493.146972656,\n",
       " 5922349.145812988,\n",
       " 5942294.368652344,\n",
       " 5934243.13269043,\n",
       " 5911132.954589844,\n",
       " 5937907.349731445,\n",
       " 5915986.329528809,\n",
       " 5926926.460876465,\n",
       " 5924002.732177734,\n",
       " 5917698.34777832]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_mask = np.isnan(data) #when calculating the train/test set to \"nan\" all the examples that are for testing so that you do not train on them \n",
    "print(torch.from_numpy(nan_mask) )\n",
    "test = PMF_zero_NB(train=data, dim=100)\n",
    "test.train_SVI(data, ~torch.from_numpy(nan_mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UA: (1000, 1, 1127, 100)\n",
      "p: (1000, 1, 1127)\n",
      "VA: (1000, 1, 5237, 100)\n",
      "target: (1000, 1127, 5237)\n",
      "[[[ 0.  0.  0. ...  0.  3.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]\n",
      "  ...\n",
      "  [25.  0.  1. ...  5. 14.  1.]\n",
      "  [ 0.  0.  0. ...  1.  6.  1.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0. ...  0.  2.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]\n",
      "  [ 2.  0.  0. ...  0.  0.  0.]\n",
      "  ...\n",
      "  [ 3.  0.  0. ... 13.  7.  0.]\n",
      "  [ 0.  0.  0. ...  1.  3.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      " [[ 2.  1.  0. ...  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]\n",
      "  [ 0.  1.  0. ...  7.  0.  0.]\n",
      "  ...\n",
      "  [ 0.  2.  0. ... 10. 49.  0.]\n",
      "  [12.  0.  0. ...  3. 39.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]\n",
      "  [ 2.  1.  1. ...  2.  7.  0.]\n",
      "  ...\n",
      "  [ 0.  0.  0. ... 10.  0.  0.]\n",
      "  [ 2.  0.  0. ...  0. 13.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...  0.  0.  1.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  1.]\n",
      "  ...\n",
      "  [ 0.  0.  0. ...  3.  7.  2.]\n",
      "  [ 2.  2.  0. ...  1. 19.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0. ...  1.  0.  0.]\n",
      "  [ 0.  0.  0. ...  5.  0.  0.]\n",
      "  [ 0.  0.  1. ...  3.  0.  1.]\n",
      "  ...\n",
      "  [35.  0.  2. ... 26.  9.  0.]\n",
      "  [ 1.  0.  0. ...  2. 45.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "test.sample_predict(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMF MAP training RMSE: 0.34168\n",
      "AUC: 0.83550\n",
      "(array([[[ 0.,  0.,  0., ...,  0.,  3.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        ...,\n",
      "        [25.,  0.,  1., ...,  5., 14.,  1.],\n",
      "        [ 0.,  0.,  0., ...,  1.,  6.,  1.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
      "\n",
      "       [[ 0.,  1.,  0., ...,  0.,  2.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 2.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 3.,  0.,  0., ..., 13.,  7.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  1.,  3.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
      "\n",
      "       [[ 2.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0., ...,  7.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  2.,  0., ..., 10., 49.,  0.],\n",
      "        [12.,  0.,  0., ...,  3., 39.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 2.,  1.,  1., ...,  2.,  7.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
      "        [ 2.,  0.,  0., ...,  0., 13.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
      "\n",
      "       [[ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  0., ...,  3.,  7.,  2.],\n",
      "        [ 2.,  2.,  0., ...,  1., 19.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
      "\n",
      "       [[ 1.,  0.,  0., ...,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
      "        [ 0.,  0.,  1., ...,  3.,  0.,  1.],\n",
      "        ...,\n",
      "        [35.,  0.,  2., ..., 26.,  9.,  0.],\n",
      "        [ 1.,  0.,  0., ...,  2., 45.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 1., 1., 0.],\n",
      "       ...,\n",
      "       [1., 0., 0., ..., 1., 1., 0.],\n",
      "       [1., 0., 0., ..., 1., 1., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))\n",
      "[[ 1  0  0 ...  1  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 1  0  0 ...  1  8  0]\n",
      " ...\n",
      " [ 8  0  0 ... 10 12  0]\n",
      " [ 1  0  0 ...  4 25  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "test.rmse(data)\n",
    "print(test.get_predictions())\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PMF_NB_with_drug_varying_alpha(nn.Module):\n",
    "    # by default our latent space is 50-dimensional\n",
    "    # and we use 400 hidden units\n",
    "    def __init__(self, train, dim):\n",
    "        super().__init__()\n",
    "        \"\"\"Build the Probabilistic Matrix Factorization model using pymc3.\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        self.dim = dim   \n",
    "        self.data = train.copy()\n",
    "        self.n, self.m = self.data.shape\n",
    "        self.map = None\n",
    "        self.bounds = (0,1)\n",
    "        self.losses = None\n",
    "        self.predictions = None\n",
    "        self.returned = None\n",
    "\n",
    "\n",
    "        # Perform mean value imputation\n",
    "    \n",
    "        \n",
    "        # Low precision reflects uncertainty; prevents overfitting.\n",
    "        # Set to the mean variance across users and items.\n",
    "        self.alpha_u = (np.mean(self.data, axis=1).mean())**2 / np.std(self.data, axis=1).mean()\n",
    "        self.alpha_v = (np.mean(self.data, axis=0).mean())**2 / np.std(self.data, axis=0).mean()\n",
    "        \n",
    "        self.beta_u = (np.mean(self.data, axis=1).mean()) / np.std(self.data, axis=1).mean()\n",
    "        self.beta_v = (np.mean(self.data, axis=0).mean()) / np.std(self.data, axis=0).mean()\n",
    "        self.bias = self.data.mean()\n",
    "\n",
    "\n",
    "    def model(self, train, mask):\n",
    "        a = 50\n",
    "\n",
    "        drug_plate = pyro.plate(\"drug_latents\", self.n, dim= -1) #independent users\n",
    "        sideeffect_plate = pyro.plate(\"sideeffect_latents\", self.m, dim= -1) #independent items\n",
    "\n",
    "        with drug_plate: \n",
    "            UA = pyro.sample(\"UA\", dist.Gamma(self.alpha_u, self.beta_u).expand([self.dim]).to_event(1))\n",
    "            #UA_int = pyro.sample(\"UAint\", dist.Normal(0., 1.))\n",
    "            alpha = pyro.sample(\"alpha\", dist.Gamma(a,a))\n",
    "        \n",
    "        with sideeffect_plate:\n",
    "            VA = pyro.sample(\"VA\", dist.Gamma(self.alpha_v, self.beta_v).expand([self.dim]).to_event(1))\n",
    "            #possibly add intercepts VA_int = pyro.sample(\"VA\", dist.Normal(0., 1.).to_event(1))\n",
    "       \n",
    "        u2_plate = pyro.plate(\"u2_plate\", self.n, dim=-2)\n",
    "\n",
    "        with sideeffect_plate, u2_plate: \n",
    "            with pyro.poutine.mask(mask=mask):\n",
    "             Y = pyro.sample(\"target\", dist.NegativeBinomial(alpha[:, np.newaxis], UA@VA.T/( UA@VA.T+alpha[:, np.newaxis]) ), obs=train ) \n",
    "             return Y\n",
    "        \n",
    "\n",
    "    def guide(self, train=None, mask=None):\n",
    "\n",
    "        d_alpha = pyro.param('d_alpha', torch.ones(self.n,self.dim), constraint=constraints.positive)#*self.user_mean)\n",
    "        d_beta = pyro.param('d_beta', 0.5*torch.ones(self.n,self.dim), constraint=constraints.positive)\n",
    "        p_alpha = pyro.param('p_alpha', 50*torch.ones(self.n), constraint=constraints.positive)\n",
    "\n",
    "        s_alpha = pyro.param('s_alpha', torch.ones(self.m,self.dim), constraint=constraints.positive)#*self.item_mean)\n",
    "        s_beta = pyro.param('s_beta', 0.5*torch.ones(self.m,self.dim), constraint=constraints.positive)\n",
    "        drug_plate = pyro.plate(\"drug_latents\", self.n, dim= -1) #independent users\n",
    "        sideeffect_plate = pyro.plate(\"sideeffect_latents\", self.m, dim= -1) #independent items\n",
    "\n",
    "        with drug_plate: \n",
    "            UA = pyro.sample(\"UA\", dist.Gamma(d_alpha, d_beta).to_event(1))\n",
    "            alpha = pyro.sample(\"alpha\", dist.Poisson(p_alpha))\n",
    "        with sideeffect_plate: \n",
    "            VA = pyro.sample(\"VA\", dist.Gamma(s_alpha, s_beta).to_event(1))\n",
    "    \n",
    "    def train_SVI(self,train,mask, nsteps=250, lr = 0.05, lrd = 1):\n",
    "        logging.basicConfig(format='%(message)s', level=logging.INFO)\n",
    "        svi = SVI(self.model,\n",
    "        self.guide,\n",
    "        optim.ClippedAdam({\"lr\": lr, \"lrd\": lrd}),\n",
    "        loss=Trace_ELBO())\n",
    "        losses = []\n",
    "        for step in range(nsteps):\n",
    "            elbo = svi.step(torch.from_numpy(train).float(), mask)\n",
    "            losses.append(elbo)\n",
    "            if step % 10 == 0:\n",
    "                print(\"Elbo loss: {}\".format(elbo))\n",
    "        self.losses = losses\n",
    "        #constrained_params = list(pyro.get_param_store().values())\n",
    "        #PARAMS = [p.unconstrained() for p in constrained_params]\n",
    "        #print(PARAMS)\n",
    "        return losses\n",
    "    \n",
    "    def sample_predict(self, nsamples=500 , verbose=True):\n",
    "        unmasked =torch.ones((self.n,self.m), dtype=torch.bool)\n",
    "        predictive_svi = Predictive(self.model, guide=self.guide, num_samples=nsamples)(None , unmasked)\n",
    "        if (verbose):\n",
    "            for k, v in predictive_svi.items():\n",
    "                print(f\"{k}: {tuple(v.shape)}\")\n",
    "        table = predictive_svi[\"target\"].numpy()\n",
    "        print(table)\n",
    "        self.returned = table\n",
    "        mc_table = table.mean(axis = 0)\n",
    "        mc_table_std = table.std(axis = 0)\n",
    "        mc_table[mc_table < self.bounds[1]] = self.bounds[0]\n",
    "        mc_table[mc_table >= self.bounds[1]] = self.bounds[1]\n",
    "        self.predictions = mc_table\n",
    "        \n",
    "    \n",
    "    def rmse(self,test):\n",
    "        low, high = self.bounds\n",
    "        test_data = test.copy()\n",
    "        test_data[test_data < high] = low\n",
    "        test_data[test_data >= high] = high\n",
    "        sqerror = abs(test_data - self.predictions) ** 2  # squared error array\n",
    "        mse = sqerror.sum()/(test_data.shape[0]*test_data.shape[1])\n",
    "        print(\"PMF MAP training RMSE: %.5f\" % np.sqrt(mse))\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(test_data.astype(int).flatten(),  self.predictions.astype(int).flatten(), pos_label=1)\n",
    "        metrics.auc(fpr, tpr)\n",
    "        print(\"AUC: %.5f\" % metrics.auc(fpr, tpr))\n",
    "        return np.sqrt(mse) , metrics.auc(fpr, tpr)\n",
    "\n",
    "    def get_predictions(self):\n",
    "        return (self.returned,self.predictions)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False]])\n",
      "Elbo loss: 624782154.984375\n",
      "Elbo loss: 210031890.42333984\n",
      "Elbo loss: 57739272.76611328\n",
      "Elbo loss: 27311795.94580078\n",
      "Elbo loss: 19653596.55517578\n",
      "Elbo loss: 17916772.713378906\n",
      "Elbo loss: 17202988.59033203\n",
      "Elbo loss: 16705030.41381836\n",
      "Elbo loss: 15988915.490234375\n",
      "Elbo loss: 15020519.251220703\n",
      "Elbo loss: 14018313.538574219\n",
      "Elbo loss: 13170151.506835938\n",
      "Elbo loss: 12490068.3203125\n",
      "Elbo loss: 12174545.774658203\n",
      "Elbo loss: 11835679.22265625\n",
      "Elbo loss: 11559356.052734375\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter probs (Tensor of shape (1127, 5237)) of distribution NegativeBinomial(total_count: torch.Size([1127, 5237]), probs: torch.Size([1127, 5237])) to satisfy the constraint HalfOpenInterval(lower_bound=0.0, upper_bound=1.0), but found invalid values:\ntensor([[1.4882e-02, 1.0651e-03, 6.1118e-04,  ..., 1.5020e-02, 1.5903e-02,\n         5.6262e-04],\n        [1.8981e-02, 1.9679e-03, 1.0406e-03,  ..., 4.3328e-02, 8.6587e-02,\n         3.1753e-03],\n        [2.7582e-02, 4.9676e-03, 2.5269e-03,  ..., 2.6313e-02, 3.7693e-02,\n         1.9212e-03],\n        ...,\n        [2.9804e-02, 6.3317e-04, 7.6498e-04,  ..., 1.3111e-02, 1.3234e-01,\n         8.8866e-04],\n        [8.2196e-02, 2.7713e-03, 4.4483e-03,  ..., 8.5888e-02, 4.6395e-01,\n         2.1183e-02],\n        [1.2121e-02, 4.0922e-04, 3.4167e-04,  ..., 1.0375e-02, 1.7970e-02,\n         2.7147e-04]], grad_fn=<DivBackward0>)\n          Trace Shapes:           \n           Param Sites:           \n          Sample Sites:           \n      drug_latents dist      |    \n                  value 1127 |    \nsideeffect_latents dist      |    \n                  value 5237 |    \n                UA dist 1127 | 100\n                  value 1127 | 100\n             alpha dist 1127 |    \n                  value 1127 |    \n                VA dist 5237 | 100\n                  value 5237 | 100\n          u2_plate dist      |    \n                  value 1127 |    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyro/poutine/trace_messenger.py:174\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyro/poutine/messenger.py:12\u001b[0m, in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mwith\u001b[39;00m context:\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/Users/dafnep/Documents/GitHub/Baysian-Matrix-Factorization-Drug-effect/src/simple_SVI_Negative_Binomial.ipynb Cell 17\u001b[0m in \u001b[0;36mPMF_NB_with_drug_varying_alpha.model\u001b[0;34m(self, train, mask)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dafnep/Documents/GitHub/Baysian-Matrix-Factorization-Drug-effect/src/simple_SVI_Negative_Binomial.ipynb#X23sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mwith\u001b[39;00m pyro\u001b[39m.\u001b[39mpoutine\u001b[39m.\u001b[39mmask(mask\u001b[39m=\u001b[39mmask):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dafnep/Documents/GitHub/Baysian-Matrix-Factorization-Drug-effect/src/simple_SVI_Negative_Binomial.ipynb#X23sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m  Y \u001b[39m=\u001b[39m pyro\u001b[39m.\u001b[39msample(\u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m, dist\u001b[39m.\u001b[39;49mNegativeBinomial(alpha[:, np\u001b[39m.\u001b[39;49mnewaxis], UA\u001b[39m@VA\u001b[39;49m\u001b[39m.\u001b[39;49mT\u001b[39m/\u001b[39;49m( UA\u001b[39m@VA\u001b[39;49m\u001b[39m.\u001b[39;49mT\u001b[39m+\u001b[39;49malpha[:, np\u001b[39m.\u001b[39;49mnewaxis]) ), obs\u001b[39m=\u001b[39mtrain ) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dafnep/Documents/GitHub/Baysian-Matrix-Factorization-Drug-effect/src/simple_SVI_Negative_Binomial.ipynb#X23sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m  \u001b[39mreturn\u001b[39;00m Y\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyro/distributions/distribution.py:18\u001b[0m, in \u001b[0;36mDistributionMeta.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m---> 18\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/distributions/negative_binomial.py:39\u001b[0m, in \u001b[0;36mNegativeBinomial.__init__\u001b[0;34m(self, total_count, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     38\u001b[0m batch_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()\n\u001b[0;32m---> 39\u001b[0m \u001b[39msuper\u001b[39;49m(NegativeBinomial, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/distributions/distribution.py:55\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m---> 55\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     56\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto satisfy the constraint \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(constraint)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m \u001b[39msuper\u001b[39m(Distribution, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter probs (Tensor of shape (1127, 5237)) of distribution NegativeBinomial(total_count: torch.Size([1127, 5237]), probs: torch.Size([1127, 5237])) to satisfy the constraint HalfOpenInterval(lower_bound=0.0, upper_bound=1.0), but found invalid values:\ntensor([[1.4882e-02, 1.0651e-03, 6.1118e-04,  ..., 1.5020e-02, 1.5903e-02,\n         5.6262e-04],\n        [1.8981e-02, 1.9679e-03, 1.0406e-03,  ..., 4.3328e-02, 8.6587e-02,\n         3.1753e-03],\n        [2.7582e-02, 4.9676e-03, 2.5269e-03,  ..., 2.6313e-02, 3.7693e-02,\n         1.9212e-03],\n        ...,\n        [2.9804e-02, 6.3317e-04, 7.6498e-04,  ..., 1.3111e-02, 1.3234e-01,\n         8.8866e-04],\n        [8.2196e-02, 2.7713e-03, 4.4483e-03,  ..., 8.5888e-02, 4.6395e-01,\n         2.1183e-02],\n        [1.2121e-02, 4.0922e-04, 3.4167e-04,  ..., 1.0375e-02, 1.7970e-02,\n         2.7147e-04]], grad_fn=<DivBackward0>)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/dafnep/Documents/GitHub/Baysian-Matrix-Factorization-Drug-effect/src/simple_SVI_Negative_Binomial.ipynb Cell 17\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dafnep/Documents/GitHub/Baysian-Matrix-Factorization-Drug-effect/src/simple_SVI_Negative_Binomial.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39mfrom_numpy(nan_mask) )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dafnep/Documents/GitHub/Baysian-Matrix-Factorization-Drug-effect/src/simple_SVI_Negative_Binomial.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m test \u001b[39m=\u001b[39m PMF_NB_with_drug_varying_alpha(train\u001b[39m=\u001b[39mdata, dim\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dafnep/Documents/GitHub/Baysian-Matrix-Factorization-Drug-effect/src/simple_SVI_Negative_Binomial.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m test\u001b[39m.\u001b[39;49mtrain_SVI(data, \u001b[39m~\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfrom_numpy(nan_mask))\n",
      "\u001b[1;32m/Users/dafnep/Documents/GitHub/Baysian-Matrix-Factorization-Drug-effect/src/simple_SVI_Negative_Binomial.ipynb Cell 17\u001b[0m in \u001b[0;36mPMF_NB_with_drug_varying_alpha.train_SVI\u001b[0;34m(self, train, mask, nsteps, lr, lrd)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dafnep/Documents/GitHub/Baysian-Matrix-Factorization-Drug-effect/src/simple_SVI_Negative_Binomial.ipynb#X23sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m losses \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dafnep/Documents/GitHub/Baysian-Matrix-Factorization-Drug-effect/src/simple_SVI_Negative_Binomial.ipynb#X23sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nsteps):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dafnep/Documents/GitHub/Baysian-Matrix-Factorization-Drug-effect/src/simple_SVI_Negative_Binomial.ipynb#X23sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m     elbo \u001b[39m=\u001b[39m svi\u001b[39m.\u001b[39;49mstep(torch\u001b[39m.\u001b[39;49mfrom_numpy(train)\u001b[39m.\u001b[39;49mfloat(), mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dafnep/Documents/GitHub/Baysian-Matrix-Factorization-Drug-effect/src/simple_SVI_Negative_Binomial.ipynb#X23sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     losses\u001b[39m.\u001b[39mappend(elbo)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dafnep/Documents/GitHub/Baysian-Matrix-Factorization-Drug-effect/src/simple_SVI_Negative_Binomial.ipynb#X23sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyro/infer/svi.py:145\u001b[0m, in \u001b[0;36mSVI.step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39m# get loss and compute gradients\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39mwith\u001b[39;00m poutine\u001b[39m.\u001b[39mtrace(param_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m param_capture:\n\u001b[0;32m--> 145\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_and_grads(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mguide, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    147\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\n\u001b[1;32m    148\u001b[0m     site[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39munconstrained() \u001b[39mfor\u001b[39;00m site \u001b[39min\u001b[39;00m param_capture\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39mnodes\u001b[39m.\u001b[39mvalues()\n\u001b[1;32m    149\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[39m# actually perform gradient steps\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[39m# torch.optim objects gets instantiated for any params that haven't been seen yet\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyro/infer/trace_elbo.py:140\u001b[0m, in \u001b[0;36mTrace_ELBO.loss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    139\u001b[0m \u001b[39m# grab a trace from the generator\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[39mfor\u001b[39;00m model_trace, guide_trace \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_traces(model, guide, args, kwargs):\n\u001b[1;32m    141\u001b[0m     loss_particle, surrogate_loss_particle \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_differentiable_loss_particle(\n\u001b[1;32m    142\u001b[0m         model_trace, guide_trace\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    144\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_particle \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_particles\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyro/infer/elbo.py:182\u001b[0m, in \u001b[0;36mELBO._get_traces\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_particles):\n\u001b[0;32m--> 182\u001b[0m         \u001b[39myield\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_trace(model, guide, args, kwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyro/infer/trace_elbo.py:57\u001b[0m, in \u001b[0;36mTrace_ELBO._get_trace\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_trace\u001b[39m(\u001b[39mself\u001b[39m, model, guide, args, kwargs):\n\u001b[1;32m     53\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39m    Returns a single trace from the guide, and the model that is run\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m    against it.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     model_trace, guide_trace \u001b[39m=\u001b[39m get_importance_trace(\n\u001b[1;32m     58\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mflat\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_plate_nesting, model, guide, args, kwargs\n\u001b[1;32m     59\u001b[0m     )\n\u001b[1;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m is_validation_enabled():\n\u001b[1;32m     61\u001b[0m         check_if_enumerated(guide_trace)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyro/infer/enum.py:65\u001b[0m, in \u001b[0;36mget_importance_trace\u001b[0;34m(graph_type, max_plate_nesting, model, guide, args, kwargs, detach)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[39mif\u001b[39;00m detach:\n\u001b[1;32m     64\u001b[0m         guide_trace\u001b[39m.\u001b[39mdetach_()\n\u001b[0;32m---> 65\u001b[0m     model_trace \u001b[39m=\u001b[39m poutine\u001b[39m.\u001b[39;49mtrace(\n\u001b[1;32m     66\u001b[0m         poutine\u001b[39m.\u001b[39;49mreplay(model, trace\u001b[39m=\u001b[39;49mguide_trace), graph_type\u001b[39m=\u001b[39;49mgraph_type\n\u001b[1;32m     67\u001b[0m     )\u001b[39m.\u001b[39;49mget_trace(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     69\u001b[0m \u001b[39mif\u001b[39;00m is_validation_enabled():\n\u001b[1;32m     70\u001b[0m     check_model_guide_match(model_trace, guide_trace, max_plate_nesting)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyro/poutine/trace_messenger.py:198\u001b[0m, in \u001b[0;36mTraceHandler.get_trace\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_trace\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    191\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[39m    :returns: data structure\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[39m    :rtype: pyro.poutine.Trace\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39m    Calls this poutine and returns its trace instead of the function's return value.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmsngr\u001b[39m.\u001b[39mget_trace()\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyro/poutine/trace_messenger.py:180\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m         exc \u001b[39m=\u001b[39m exc_type(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(exc_value, shapes))\n\u001b[1;32m    179\u001b[0m         exc \u001b[39m=\u001b[39m exc\u001b[39m.\u001b[39mwith_traceback(traceback)\n\u001b[0;32m--> 180\u001b[0m         \u001b[39mraise\u001b[39;00m exc \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmsngr\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39madd_node(\n\u001b[1;32m    182\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m_RETURN\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_RETURN\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreturn\u001b[39m\u001b[39m\"\u001b[39m, value\u001b[39m=\u001b[39mret\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    184\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyro/poutine/trace_messenger.py:174\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmsngr\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39madd_node(\n\u001b[1;32m    171\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m_INPUT\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_INPUT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[1;32m    172\u001b[0m )\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    176\u001b[0m     exc_type, exc_value, traceback \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyro/poutine/messenger.py:12\u001b[0m, in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_context_wrap\u001b[39m(context, fn, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     11\u001b[0m     \u001b[39mwith\u001b[39;00m context:\n\u001b[0;32m---> 12\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/Users/dafnep/Documents/GitHub/Baysian-Matrix-Factorization-Drug-effect/src/simple_SVI_Negative_Binomial.ipynb Cell 17\u001b[0m in \u001b[0;36mPMF_NB_with_drug_varying_alpha.model\u001b[0;34m(self, train, mask)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dafnep/Documents/GitHub/Baysian-Matrix-Factorization-Drug-effect/src/simple_SVI_Negative_Binomial.ipynb#X23sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mwith\u001b[39;00m sideeffect_plate, u2_plate: \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dafnep/Documents/GitHub/Baysian-Matrix-Factorization-Drug-effect/src/simple_SVI_Negative_Binomial.ipynb#X23sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39mwith\u001b[39;00m pyro\u001b[39m.\u001b[39mpoutine\u001b[39m.\u001b[39mmask(mask\u001b[39m=\u001b[39mmask):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dafnep/Documents/GitHub/Baysian-Matrix-Factorization-Drug-effect/src/simple_SVI_Negative_Binomial.ipynb#X23sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m      Y \u001b[39m=\u001b[39m pyro\u001b[39m.\u001b[39msample(\u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m, dist\u001b[39m.\u001b[39;49mNegativeBinomial(alpha[:, np\u001b[39m.\u001b[39;49mnewaxis], UA\u001b[39m@VA\u001b[39;49m\u001b[39m.\u001b[39;49mT\u001b[39m/\u001b[39;49m( UA\u001b[39m@VA\u001b[39;49m\u001b[39m.\u001b[39;49mT\u001b[39m+\u001b[39;49malpha[:, np\u001b[39m.\u001b[39;49mnewaxis]) ), obs\u001b[39m=\u001b[39mtrain ) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dafnep/Documents/GitHub/Baysian-Matrix-Factorization-Drug-effect/src/simple_SVI_Negative_Binomial.ipynb#X23sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m      \u001b[39mreturn\u001b[39;00m Y\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyro/distributions/distribution.py:18\u001b[0m, in \u001b[0;36mDistributionMeta.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m---> 18\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/distributions/negative_binomial.py:39\u001b[0m, in \u001b[0;36mNegativeBinomial.__init__\u001b[0;34m(self, total_count, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprobs \u001b[39mif\u001b[39;00m probs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     38\u001b[0m batch_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()\n\u001b[0;32m---> 39\u001b[0m \u001b[39msuper\u001b[39;49m(NegativeBinomial, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/distributions/distribution.py:55\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     53\u001b[0m         valid \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39mcheck(value)\n\u001b[1;32m     54\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m---> 55\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     56\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto satisfy the constraint \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(constraint)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m \u001b[39msuper\u001b[39m(Distribution, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter probs (Tensor of shape (1127, 5237)) of distribution NegativeBinomial(total_count: torch.Size([1127, 5237]), probs: torch.Size([1127, 5237])) to satisfy the constraint HalfOpenInterval(lower_bound=0.0, upper_bound=1.0), but found invalid values:\ntensor([[1.4882e-02, 1.0651e-03, 6.1118e-04,  ..., 1.5020e-02, 1.5903e-02,\n         5.6262e-04],\n        [1.8981e-02, 1.9679e-03, 1.0406e-03,  ..., 4.3328e-02, 8.6587e-02,\n         3.1753e-03],\n        [2.7582e-02, 4.9676e-03, 2.5269e-03,  ..., 2.6313e-02, 3.7693e-02,\n         1.9212e-03],\n        ...,\n        [2.9804e-02, 6.3317e-04, 7.6498e-04,  ..., 1.3111e-02, 1.3234e-01,\n         8.8866e-04],\n        [8.2196e-02, 2.7713e-03, 4.4483e-03,  ..., 8.5888e-02, 4.6395e-01,\n         2.1183e-02],\n        [1.2121e-02, 4.0922e-04, 3.4167e-04,  ..., 1.0375e-02, 1.7970e-02,\n         2.7147e-04]], grad_fn=<DivBackward0>)\n          Trace Shapes:           \n           Param Sites:           \n          Sample Sites:           \n      drug_latents dist      |    \n                  value 1127 |    \nsideeffect_latents dist      |    \n                  value 5237 |    \n                UA dist 1127 | 100\n                  value 1127 | 100\n             alpha dist 1127 |    \n                  value 1127 |    \n                VA dist 5237 | 100\n                  value 5237 | 100\n          u2_plate dist      |    \n                  value 1127 |    "
     ]
    }
   ],
   "source": [
    "nan_mask = np.isnan(data) #when calculating the train/test set to \"nan\" all the examples that are for testing so that you do not train on them \n",
    "print(torch.from_numpy(nan_mask) )\n",
    "test = PMF_NB_with_drug_varying_alpha(train=data, dim=100)\n",
    "test.train_SVI(data, ~torch.from_numpy(nan_mask))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
