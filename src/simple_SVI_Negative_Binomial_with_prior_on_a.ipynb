{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from torch.distributions import constraints\n",
    "from torch import nn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import Predictive\n",
    "import seaborn as sns\n",
    "from pyro import poutine\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1127, 5237)\n",
      "tensor([[False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('data_all.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "print(data.shape)\n",
    "\n",
    "nan_mask = np.isnan(data) #when calculating the train/test set to \"nan\" all the examples that are for testing so that you do not train on them \n",
    "print(torch.from_numpy(nan_mask) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PMF_NB_with_drug_varying_alpha(nn.Module):\n",
    "    # by default our latent space is 50-dimensional\n",
    "    # and we use 400 hidden units\n",
    "    def __init__(self, train, dim):\n",
    "        super().__init__()\n",
    "        \"\"\"Build the Probabilistic Matrix Factorization model using pymc3.\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        self.dim = dim   \n",
    "        self.data = train.copy()\n",
    "        self.n, self.m = self.data.shape\n",
    "        self.map = None\n",
    "        self.bounds = (0,1)\n",
    "        self.losses = None\n",
    "        self.predictions = None\n",
    "        self.returned = None\n",
    "\n",
    "\n",
    "        # Perform mean value imputation\n",
    "    \n",
    "        \n",
    "        # Low precision reflects uncertainty; prevents overfitting.\n",
    "        # Set to the mean variance across users and items.\n",
    "        self.alpha_u = (np.mean(self.data, axis=1).mean())**2 / np.std(self.data, axis=1).mean()\n",
    "        self.alpha_v = (np.mean(self.data, axis=0).mean())**2 / np.std(self.data, axis=0).mean()\n",
    "        \n",
    "        self.beta_u = (np.mean(self.data, axis=1).mean()) / np.std(self.data, axis=1).mean()\n",
    "        self.beta_v = (np.mean(self.data, axis=0).mean()) / np.std(self.data, axis=0).mean()\n",
    "        self.bias = self.data.mean()\n",
    "\n",
    "\n",
    "    def model(self, train, mask):\n",
    "        a = 50\n",
    "\n",
    "        drug_plate = pyro.plate(\"drug_latents\", self.n, dim= -1) #independent users\n",
    "        sideeffect_plate = pyro.plate(\"sideeffect_latents\", self.m, dim= -1) #independent items\n",
    "\n",
    "        with drug_plate: \n",
    "            UA = pyro.sample(\"UA\", dist.Gamma(self.alpha_u, self.beta_u).expand([self.dim]).to_event(1))\n",
    "            #UA_int = pyro.sample(\"UAint\", dist.Normal(0., 1.))\n",
    "            exposure = pyro.sample(\"exposure\", dist.Gamma(a,a))\n",
    "        \n",
    "        with sideeffect_plate:\n",
    "            VA = pyro.sample(\"VA\", dist.Gamma(self.alpha_v, self.beta_v).expand([self.dim]).to_event(1))\n",
    "            #possibly add intercepts VA_int = pyro.sample(\"VA\", dist.Normal(0., 1.).to_event(1))\n",
    "       \n",
    "        u2_plate = pyro.plate(\"u2_plate\", self.n, dim=-2)\n",
    "\n",
    "        with sideeffect_plate, u2_plate: \n",
    "            with pyro.poutine.mask(mask=mask):\n",
    "             Y = pyro.sample(\"target\", dist.Poisson(exposure[:, np.newaxis]*(UA@VA.T )), obs=train ) \n",
    "             return Y\n",
    "        \n",
    "\n",
    "    def guide(self, train=None, mask=None):\n",
    "\n",
    "        d_alpha = pyro.param('d_alpha', torch.ones(self.n,self.dim), constraint=constraints.positive)#*self.user_mean)\n",
    "        d_beta = pyro.param('d_beta', 0.5*torch.ones(self.n,self.dim), constraint=constraints.positive)\n",
    "        exp_alpha = pyro.param('exp_alpha', 10*torch.ones(self.n), constraint=constraints.positive)\n",
    "\n",
    "        s_alpha = pyro.param('s_alpha', torch.ones(self.m,self.dim), constraint=constraints.positive)#*self.item_mean)\n",
    "        s_beta = pyro.param('s_beta', 0.5*torch.ones(self.m,self.dim), constraint=constraints.positive)\n",
    "        drug_plate = pyro.plate(\"drug_latents\", self.n, dim= -1) #independent users\n",
    "        sideeffect_plate = pyro.plate(\"sideeffect_latents\", self.m, dim= -1) #independent items\n",
    "\n",
    "        with drug_plate: \n",
    "            UA = pyro.sample(\"UA\", dist.Gamma(d_alpha, d_beta).to_event(1))\n",
    "            exposure = pyro.sample(\"exposure\", dist.Gamma(exp_alpha,exp_alpha))\n",
    "        with sideeffect_plate: \n",
    "            VA = pyro.sample(\"VA\", dist.Gamma(s_alpha, s_beta).to_event(1))\n",
    "    \n",
    "    def train_SVI(self,train,mask, nsteps=250, lr = 0.05, lrd = 1):\n",
    "        logging.basicConfig(format='%(message)s', level=logging.INFO)\n",
    "        svi = SVI(self.model,\n",
    "        self.guide,\n",
    "        optim.ClippedAdam({\"lr\": lr, \"lrd\": lrd}),\n",
    "        loss=Trace_ELBO())\n",
    "        losses = []\n",
    "        for step in range(nsteps):\n",
    "            elbo = svi.step(torch.from_numpy(train).float(), mask)\n",
    "            losses.append(elbo)\n",
    "            if step % 10 == 0:\n",
    "                print(\"Elbo loss: {}\".format(elbo))\n",
    "        self.losses = losses\n",
    "        #constrained_params = list(pyro.get_param_store().values())\n",
    "        #PARAMS = [p.unconstrained() for p in constrained_params]\n",
    "        #print(PARAMS)\n",
    "        return losses\n",
    "    \n",
    "    def sample_predict(self, nsamples=500 , verbose=True):\n",
    "        unmasked =torch.ones((self.n,self.m), dtype=torch.bool)\n",
    "        predictive_svi = Predictive(self.model, guide=self.guide, num_samples=nsamples)(None , unmasked)\n",
    "        if (verbose):\n",
    "            for k, v in predictive_svi.items():\n",
    "                print(f\"{k}: {tuple(v.shape)}\")\n",
    "        table = predictive_svi[\"exposure\"].numpy()\n",
    "        print(table)\n",
    "        self.returned = table\n",
    "        mc_table = table.mean(axis = 0)\n",
    "        mc_table_std = table.std(axis = 0)\n",
    "        mc_table[mc_table < self.bounds[1]] = self.bounds[0]\n",
    "        mc_table[mc_table >= self.bounds[1]] = self.bounds[1]\n",
    "        self.predictions = mc_table\n",
    "        \n",
    "    \n",
    "    def rmse(self,test):\n",
    "        low, high = self.bounds\n",
    "        test_data = test.copy()\n",
    "        test_data[test_data < high] = low\n",
    "        test_data[test_data >= high] = high\n",
    "        sqerror = abs(test_data - self.predictions) ** 2  # squared error array\n",
    "        mse = sqerror.sum()/(test_data.shape[0]*test_data.shape[1])\n",
    "        print(\"PMF MAP training RMSE: %.5f\" % np.sqrt(mse))\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(test_data.astype(int).flatten(),  self.predictions.astype(int).flatten(), pos_label=1)\n",
    "        metrics.auc(fpr, tpr)\n",
    "        print(\"AUC: %.5f\" % metrics.auc(fpr, tpr))\n",
    "        return np.sqrt(mse) , metrics.auc(fpr, tpr)\n",
    "\n",
    "    def get_predictions(self):\n",
    "        return (self.returned,self.predictions)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False]])\n",
      "Elbo loss: 2299921026.017166\n",
      "Elbo loss: 339659708.7041626\n",
      "Elbo loss: 89322424.35913086\n",
      "Elbo loss: 48881848.32763672\n",
      "Elbo loss: 36780380.53189087\n",
      "Elbo loss: 33859506.11453247\n",
      "Elbo loss: 32763926.523101807\n",
      "Elbo loss: 31377885.670043945\n",
      "Elbo loss: 28825171.24220276\n",
      "Elbo loss: 25865128.2783432\n",
      "Elbo loss: 22923475.63496065\n",
      "Elbo loss: 21334513.88455963\n",
      "Elbo loss: 19966221.38685608\n",
      "Elbo loss: 19174857.985290527\n",
      "Elbo loss: 18261999.525665283\n",
      "Elbo loss: 17797722.53552246\n",
      "Elbo loss: 17391315.294647217\n",
      "Elbo loss: 17144953.75350952\n",
      "Elbo loss: 16672882.913513184\n",
      "Elbo loss: 16585416.898406982\n",
      "Elbo loss: 16091566.316802979\n",
      "Elbo loss: 15865470.235534668\n",
      "Elbo loss: 15782810.784301758\n",
      "Elbo loss: 15761995.164611816\n",
      "Elbo loss: 15677222.792541504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2299921026.017166,\n",
       " 1896585878.9816132,\n",
       " 1528808577.1617126,\n",
       " 1281812323.570465,\n",
       " 1047349005.3313599,\n",
       " 862724784.6808167,\n",
       " 697222344.0084229,\n",
       " 591083613.0669861,\n",
       " 482135093.70376587,\n",
       " 400645422.370636,\n",
       " 339659708.7041626,\n",
       " 283667539.7874756,\n",
       " 238814068.64575195,\n",
       " 202139789.33895874,\n",
       " 175065781.60766602,\n",
       " 151419734.45230103,\n",
       " 134255352.4430542,\n",
       " 119533522.41479492,\n",
       " 106911101.96694946,\n",
       " 99549447.71835327,\n",
       " 89322424.35913086,\n",
       " 82408629.55587769,\n",
       " 77360420.18356323,\n",
       " 71797520.88208008,\n",
       " 68260145.6499939,\n",
       " 62933725.846466064,\n",
       " 60019663.7673645,\n",
       " 56695038.13146973,\n",
       " 53983017.905548096,\n",
       " 51202553.24920654,\n",
       " 48881848.32763672,\n",
       " 46348769.345214844,\n",
       " 44581686.32183838,\n",
       " 42931257.02807617,\n",
       " 41098875.060791016,\n",
       " 39732322.4710083,\n",
       " 38863585.153045654,\n",
       " 37800449.43481445,\n",
       " 37676300.815093994,\n",
       " 37218402.71411133,\n",
       " 36780380.53189087,\n",
       " 36662705.857177734,\n",
       " 36767195.82522583,\n",
       " 36820468.115875244,\n",
       " 36559821.24105835,\n",
       " 36053366.34210205,\n",
       " 36384098.32397461,\n",
       " 35438870.4385376,\n",
       " 35218776.693115234,\n",
       " 34813082.345184326,\n",
       " 33859506.11453247,\n",
       " 33719917.409576416,\n",
       " 33491507.846710205,\n",
       " 33601972.4888916,\n",
       " 33393707.027130127,\n",
       " 33200424.586761475,\n",
       " 33402157.76223755,\n",
       " 33255415.268798828,\n",
       " 32955063.4692688,\n",
       " 32985205.85897827,\n",
       " 32763926.523101807,\n",
       " 32475428.43222046,\n",
       " 32331591.00567627,\n",
       " 32382070.054138184,\n",
       " 32248888.612060547,\n",
       " 32028347.405807495,\n",
       " 32077796.962402344,\n",
       " 31811397.156463623,\n",
       " 31856802.174438477,\n",
       " 31391026.363952637,\n",
       " 31377885.670043945,\n",
       " 31452033.628112793,\n",
       " 30873525.165390015,\n",
       " 31113631.73135376,\n",
       " 30693055.121154785,\n",
       " 30347700.7449646,\n",
       " 30087017.916870117,\n",
       " 29881232.18017578,\n",
       " 29369406.153198242,\n",
       " 29340734.499938965,\n",
       " 28825171.24220276,\n",
       " 28576566.518066406,\n",
       " 28283961.12210846,\n",
       " 27995245.014694214,\n",
       " 27710031.50112915,\n",
       " 27424925.528404236,\n",
       " 27010178.489341736,\n",
       " 26809251.256629944,\n",
       " 26298670.326332092,\n",
       " 26376726.469638824,\n",
       " 25865128.2783432,\n",
       " 25568999.928634644,\n",
       " 25392892.847084045,\n",
       " 25041514.936279297,\n",
       " 24583802.4261055,\n",
       " 24298890.037078857,\n",
       " 24284664.809379578,\n",
       " 23955467.94756317,\n",
       " 23870251.14743805,\n",
       " 23562820.242908478,\n",
       " 22923475.63496065,\n",
       " 22936323.52779579,\n",
       " 22662431.55971527,\n",
       " 22441825.151887894,\n",
       " 22290275.119062424,\n",
       " 22304904.86602211,\n",
       " 22000481.825356483,\n",
       " 21801037.93735695,\n",
       " 21646979.62683487,\n",
       " 21604521.96544838,\n",
       " 21334513.88455963,\n",
       " 21147544.20598221,\n",
       " 21190667.137184143,\n",
       " 21004006.760681152,\n",
       " 20725570.702201843,\n",
       " 20783638.139518738,\n",
       " 20778854.61277008,\n",
       " 20294798.452575684,\n",
       " 20388432.07574463,\n",
       " 20257503.712905884,\n",
       " 19966221.38685608,\n",
       " 19722534.555503845,\n",
       " 19889197.80570984,\n",
       " 19843437.86151123,\n",
       " 19723533.403247833,\n",
       " 19610361.740585327,\n",
       " 19384290.361442566,\n",
       " 19649168.39791107,\n",
       " 19257997.12776184,\n",
       " 19255456.353019714,\n",
       " 19174857.985290527,\n",
       " 18879187.725204468,\n",
       " 19002861.54988098,\n",
       " 18895589.73626709,\n",
       " 18777567.240219116,\n",
       " 18820614.375579834,\n",
       " 18659346.332244873,\n",
       " 18577313.038223267,\n",
       " 18451717.449188232,\n",
       " 18468419.576034546,\n",
       " 18261999.525665283,\n",
       " 18317160.589813232,\n",
       " 18360037.622131348,\n",
       " 18347320.803451538,\n",
       " 18218257.67880249,\n",
       " 17925562.604431152,\n",
       " 18119198.571609497,\n",
       " 17909316.668685913,\n",
       " 17947438.225891113,\n",
       " 18012310.620887756,\n",
       " 17797722.53552246,\n",
       " 17610454.423797607,\n",
       " 17886336.867721558,\n",
       " 17727257.935073853,\n",
       " 17738620.52053833,\n",
       " 17550510.457687378,\n",
       " 17592237.00932312,\n",
       " 17509562.29685974,\n",
       " 17677578.577468872,\n",
       " 17514339.60406494,\n",
       " 17391315.294647217,\n",
       " 17469762.924850464,\n",
       " 17224301.115966797,\n",
       " 17251662.053268433,\n",
       " 17235346.353744507,\n",
       " 17307894.41142273,\n",
       " 17073526.468780518,\n",
       " 17025839.055114746,\n",
       " 17095153.309448242,\n",
       " 17072154.27645874,\n",
       " 17144953.75350952,\n",
       " 17200914.376678467,\n",
       " 17167421.565216064,\n",
       " 17010748.78916931,\n",
       " 16979214.428741455,\n",
       " 16867498.27319336,\n",
       " 17034930.310699463,\n",
       " 16666966.680984497,\n",
       " 16895551.184631348,\n",
       " 16753160.308624268,\n",
       " 16672882.913513184,\n",
       " 16548798.630615234,\n",
       " 16619051.736053467,\n",
       " 16571350.079162598,\n",
       " 16794513.35510254,\n",
       " 16643559.164916992,\n",
       " 16634559.226196289,\n",
       " 16687164.352935791,\n",
       " 16775331.749603271,\n",
       " 16415572.538787842,\n",
       " 16585416.898406982,\n",
       " 16414289.394256592,\n",
       " 16443234.181243896,\n",
       " 16496711.636138916,\n",
       " 16500070.196990967,\n",
       " 16361914.39263916,\n",
       " 16383787.48638916,\n",
       " 16431449.982452393,\n",
       " 16244136.582061768,\n",
       " 16206336.725463867,\n",
       " 16091566.316802979,\n",
       " 16171844.044677734,\n",
       " 16089996.65359497,\n",
       " 16236422.655822754,\n",
       " 16129070.559570312,\n",
       " 16252656.874908447,\n",
       " 16109352.87789917,\n",
       " 16215316.172424316,\n",
       " 15999662.755859375,\n",
       " 16319346.109313965,\n",
       " 15865470.235534668,\n",
       " 15965784.910949707,\n",
       " 16124349.392303467,\n",
       " 16296922.811767578,\n",
       " 15960945.33428955,\n",
       " 15910359.954406738,\n",
       " 15878177.55532837,\n",
       " 15863554.599609375,\n",
       " 16009187.415130615,\n",
       " 16003951.941192627,\n",
       " 15782810.784301758,\n",
       " 16108056.513519287,\n",
       " 15689369.68447876,\n",
       " 15970217.553894043,\n",
       " 15833246.01940918,\n",
       " 15978358.871765137,\n",
       " 16002080.553283691,\n",
       " 16053380.522918701,\n",
       " 15690577.657867432,\n",
       " 15909723.157562256,\n",
       " 15761995.164611816,\n",
       " 15708223.06640625,\n",
       " 15598829.886871338,\n",
       " 15730506.228637695,\n",
       " 15801630.003479004,\n",
       " 15615761.282348633,\n",
       " 15862572.521453857,\n",
       " 15415743.822021484,\n",
       " 15500853.282073975,\n",
       " 15495961.128723145,\n",
       " 15677222.792541504,\n",
       " 15695523.34564209,\n",
       " 15476583.093688965,\n",
       " 15724857.633239746,\n",
       " 15403827.40838623,\n",
       " 15421000.405395508,\n",
       " 15481551.8147583,\n",
       " 15717281.762390137,\n",
       " 15325448.211364746,\n",
       " 15104936.485717773]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_mask = np.isnan(data) #when calculating the train/test set to \"nan\" all the examples that are for testing so that you do not train on them \n",
    "print(torch.from_numpy(nan_mask) )\n",
    "test = PMF_NB_with_drug_varying_alpha(train=data, dim=100)\n",
    "test.train_SVI(data, ~torch.from_numpy(nan_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UA: (1000, 1, 1127, 100)\n",
      "exposure: (1000, 1, 1127)\n",
      "VA: (1000, 1, 5237, 100)\n",
      "target: (1000, 1127, 5237)\n",
      "[[[ 1.  0.  0. ...  1.  0.  1.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...  2.  2.  0.]\n",
      "  ...\n",
      "  [12.  0.  0. ...  6.  7.  0.]\n",
      "  [ 3.  0.  1. ...  8. 15.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...  1.  2.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]\n",
      "  [ 0.  0.  2. ...  0.  2.  0.]\n",
      "  ...\n",
      "  [14.  0.  0. ...  6.  2.  0.]\n",
      "  [ 4.  0.  0. ...  2. 25.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      " [[ 2.  0.  0. ...  0.  1.  0.]\n",
      "  [ 1.  0.  0. ...  0.  0.  0.]\n",
      "  [ 1.  0.  0. ...  0.  0.  0.]\n",
      "  ...\n",
      "  [14.  0.  1. ... 12.  4.  0.]\n",
      "  [ 7.  0.  0. ...  7.  3.  1.]\n",
      "  [ 0.  0.  0. ...  0.  1.  0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.  0.  0. ...  0.  2.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...  3. 12.  0.]\n",
      "  ...\n",
      "  [ 8.  0.  0. ... 21. 33.  0.]\n",
      "  [ 6.  0.  0. ...  6. 19.  1.]\n",
      "  [ 0.  0.  0. ...  1.  0.  0.]]\n",
      "\n",
      " [[ 1.  1.  0. ...  1.  1.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...  1.  4.  0.]\n",
      "  ...\n",
      "  [10.  0.  0. ... 13. 19.  0.]\n",
      "  [ 3.  0.  0. ...  4. 10.  0.]\n",
      "  [ 1.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...  0.  1.  0.]\n",
      "  [ 0.  0.  0. ...  0.  1.  0.]\n",
      "  [ 0.  0.  0. ...  1.  1.  0.]\n",
      "  ...\n",
      "  [ 3.  0.  1. ...  5. 21.  0.]\n",
      "  [ 4.  0.  0. ...  7. 39.  0.]\n",
      "  [ 0.  0.  0. ...  0.  0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "test.sample_predict(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMF MAP training RMSE: 0.33401\n",
      "AUC: 0.84041\n",
      "[[ 1  0  0 ...  1  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 1  0  0 ...  1  8  0]\n",
      " ...\n",
      " [ 8  0  0 ... 10 12  0]\n",
      " [ 1  0  0 ...  4 25  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "test.rmse(data)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
