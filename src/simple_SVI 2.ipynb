{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from torch.distributions import constraints\n",
    "from torch import nn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import Predictive\n",
    "import seaborn as sns\n",
    "from pyro import poutine\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PMF(nn.Module):\n",
    "    # by default our latent space is 50-dimensional\n",
    "    # and we use 400 hidden units\n",
    "    def __init__(self, data, train, dim):\n",
    "        super().__init__()\n",
    "        \"\"\"Build the Probabilistic Matrix Factorization model using pymc3.\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        self.dim = dim   \n",
    "        self.data = data.copy()\n",
    "        self.n, self.m = self.data.shape\n",
    "        self.map = None\n",
    "        self.bounds = (0,1)\n",
    "        self.losses = None\n",
    "        self.predictions = None\n",
    "        self.returned = None\n",
    "        self.predictive_svi = None\n",
    "\n",
    "\n",
    "        # Perform mean value imputation\n",
    "    \n",
    "        \n",
    "        # Low precision reflects uncertainty; prevents overfitting.\n",
    "        # Set to the mean variance across users and items.\n",
    "        self.alpha_u = (np.mean(self.data, axis=1).mean())**2 / np.std(self.data, axis=1).mean()\n",
    "        self.alpha_v = (np.mean(self.data, axis=0).mean())**2 / np.std(self.data, axis=0).mean()\n",
    "        \n",
    "        self.beta_u = (np.mean(self.data, axis=1).mean()) / np.std(self.data, axis=1).mean()\n",
    "        self.beta_v = (np.mean(self.data, axis=0).mean()) / np.std(self.data, axis=0).mean()\n",
    "        self.bias = self.data.mean()\n",
    "\n",
    "\n",
    "    def model(self, train, mask):\n",
    "     \n",
    "\n",
    "        drug_plate = pyro.plate(\"drug_latents\", self.n, dim= -1) #independent users\n",
    "        sideeffect_plate = pyro.plate(\"sideeffect_latents\", self.m, dim= -1) #independent items\n",
    "\n",
    "        with drug_plate: \n",
    "            UA = pyro.sample(\"UA\", dist.Gamma(self.alpha_u, self.beta_u).expand([self.dim]).to_event(1))\n",
    "            #UA_int = pyro.sample(\"UAint\", dist.Normal(0., 1.))\n",
    "        \n",
    "        with sideeffect_plate:\n",
    "            VA = pyro.sample(\"VA\", dist.Gamma(self.alpha_v, self.beta_v).expand([self.dim]).to_event(1))\n",
    "            #possibly add intercepts VA_int = pyro.sample(\"VA\", dist.Normal(0., 1.).to_event(1))\n",
    "       \n",
    "        u2_plate = pyro.plate(\"u2_plate\", self.n, dim=-2)\n",
    "\n",
    "        with sideeffect_plate, u2_plate: \n",
    "            with pyro.poutine.mask(mask=mask):\n",
    "             Y = pyro.sample(\"target\", dist.Poisson(UA@VA.T), obs=train ) \n",
    "             return Y\n",
    "        \n",
    "\n",
    "    def guide(self, train=None, mask=None):\n",
    "\n",
    "        d_alpha = pyro.param('d_alpha', torch.ones(self.n,self.dim), constraint=constraints.positive)#*self.user_mean)\n",
    "        d_beta = pyro.param('d_beta', 0.5*torch.ones(self.n,self.dim), constraint=constraints.positive)\n",
    "       # int_mean = pyro.param('int_mean', torch.tensor(1.)*self.user_mean)\n",
    "       # mov_cov = pyro.param('mov_cov', torch.tensor(1.)*0.1,\n",
    "          #                  constraint=constraints.positive)\n",
    "\n",
    "        s_alpha = pyro.param('s_alpha', torch.ones(self.m,self.dim), constraint=constraints.positive)#*self.item_mean)\n",
    "        s_beta = pyro.param('s_beta', 0.5*torch.ones(self.m,self.dim), constraint=constraints.positive)\n",
    "        drug_plate = pyro.plate(\"drug_latents\", self.n, dim= -1) #independent users\n",
    "        sideeffect_plate = pyro.plate(\"sideeffect_latents\", self.m, dim= -1) #independent items\n",
    "\n",
    "        with drug_plate: \n",
    "            UA = pyro.sample(\"UA\", dist.Gamma(d_alpha, d_beta).to_event(1))\n",
    "           # UA_int = pyro.sample(\"UAint\", dist.Normal(int_mean, mov_cov).to_event(1))\n",
    "        with sideeffect_plate: \n",
    "            VA = pyro.sample(\"VA\", dist.Gamma(s_alpha, s_beta).to_event(1))\n",
    "    \n",
    "    def train_SVI(self,train,mask, nsteps=250, lr = 0.05, lrd = 1, verbose=True):\n",
    "        logging.basicConfig(format='%(message)s', level=logging.INFO)\n",
    "        svi = SVI(self.model,\n",
    "        self.guide,\n",
    "        optim.ClippedAdam({\"lr\": lr, \"lrd\": lrd}),\n",
    "        loss=Trace_ELBO())\n",
    "        losses = []\n",
    "        for step in range(nsteps):\n",
    "            elbo = svi.step(torch.from_numpy(train).float(), mask)\n",
    "            losses.append(elbo)\n",
    "            if(verbose):\n",
    "                if step % 10 == 0:\n",
    "                    print(\"Elbo loss: {}\".format(elbo))\n",
    "        self.losses = losses\n",
    "        #constrained_params = list(pyro.get_param_store().values())\n",
    "        #PARAMS = [p.unconstrained() for p in constrained_params]\n",
    "        #print(PARAMS)\n",
    "        return losses\n",
    "    \n",
    "    def sample_predict(self, nsamples=500 , verbose=True):\n",
    "        unmasked =torch.ones((self.n,self.m), dtype=torch.bool)\n",
    "        predictive_svi = Predictive(self.model, guide=self.guide, num_samples=nsamples)(None , unmasked)\n",
    "        if (verbose):\n",
    "            for k, v in predictive_svi.items():\n",
    "                print(f\"{k}: {tuple(v.shape)}\")\n",
    "        table = predictive_svi[\"target\"].numpy()\n",
    "        self.predictive_svi = predictive_svi\n",
    "        #print(table)\n",
    "        self.returned = table\n",
    "        mc_table = table.mean(axis = 0)\n",
    "        mc_table_std = table.std(axis = 0)\n",
    "        mc_table[mc_table < self.bounds[1]] = self.bounds[0]\n",
    "        mc_table[mc_table >= self.bounds[1]] = self.bounds[1]\n",
    "        self.predictions = mc_table\n",
    "\n",
    "    def predictive_score(self,test,masked):\n",
    "        # total = test.shape[0]*test.shape[1]\n",
    "        \n",
    "        UA =  self.predictive_svi[\"UA\"]\n",
    "        VA = self.predictive_svi[\"VA\"]\n",
    "        VA = VA.mean(axis=0).reshape(self.m,self.dim)\n",
    "        UA = UA.mean(axis=0).reshape(self.n,self.dim)\n",
    "        print(UA.shape)\n",
    "        score = dist.Poisson(UA@VA.T).log_prob(torch.from_numpy(test))\n",
    "        mean_score = (score*masked).reshape(-1).logsumexp(-1) -np.log(test.shape[0]*test.shape[1])\n",
    "        return mean_score\n",
    "        \n",
    "    \n",
    "    def rmse(self,test):\n",
    "        low, high = self.bounds\n",
    "        test_data = test.copy()\n",
    "        test_data[test_data < high] = low\n",
    "        test_data[test_data >= high] = high\n",
    "        sqerror = abs(test_data - self.predictions) ** 2  # squared error array\n",
    "        mse = sqerror.sum()/(test_data.shape[0]*test_data.shape[1])\n",
    "        print(\"PMF MAP training RMSE: %.5f\" % np.sqrt(mse))\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(test_data.astype(int).flatten(),  self.predictions.astype(int).flatten(), pos_label=1)\n",
    "        metrics.auc(fpr, tpr)\n",
    "        print(\"AUC: %.5f\" % metrics.auc(fpr, tpr))\n",
    "        return np.sqrt(mse) , metrics.auc(fpr, tpr)\n",
    "\n",
    "    def get_predictions(self):\n",
    "        return (self.returned,self.predictions)\n",
    "\n",
    "    \n",
    "   \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1127, 5237)\n",
      "tensor([[False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('data_all.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "print(data.shape)\n",
    "nan_mask = np.isnan(data) #when calculating the train/test set to \"nan\" all the examples that are for testing so that you do not train on them \n",
    "print(torch.from_numpy(nan_mask) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1127, 5237)\n",
      "UA: (500, 1, 1127, 1)\n",
      "VA: (500, 1, 5237, 1)\n",
      "target: (500, 1127, 5237)\n",
      "torch.Size([1127, 1])\n",
      "UA: (500, 1, 1127, 5)\n",
      "VA: (500, 1, 5237, 5)\n",
      "target: (500, 1127, 5237)\n",
      "torch.Size([1127, 5])\n",
      "UA: (500, 1, 1127, 10)\n",
      "VA: (500, 1, 5237, 10)\n",
      "target: (500, 1127, 5237)\n",
      "torch.Size([1127, 10])\n",
      "UA: (500, 1, 1127, 20)\n",
      "VA: (500, 1, 5237, 20)\n",
      "target: (500, 1127, 5237)\n",
      "torch.Size([1127, 20])\n",
      "UA: (500, 1, 1127, 50)\n",
      "VA: (500, 1, 5237, 50)\n",
      "target: (500, 1127, 5237)\n",
      "torch.Size([1127, 50])\n",
      "UA: (500, 1, 1127, 75)\n",
      "VA: (500, 1, 5237, 75)\n",
      "target: (500, 1127, 5237)\n",
      "torch.Size([1127, 75])\n",
      "UA: (500, 1, 1127, 100)\n",
      "VA: (500, 1, 5237, 100)\n",
      "target: (500, 1127, 5237)\n",
      "torch.Size([1127, 100])\n",
      "UA: (500, 1, 1127, 200)\n",
      "VA: (500, 1, 5237, 200)\n",
      "target: (500, 1127, 5237)\n",
      "torch.Size([1127, 200])\n",
      "UA: (500, 1, 1127, 250)\n",
      "VA: (500, 1, 5237, 250)\n",
      "target: (500, 1127, 5237)\n",
      "torch.Size([1127, 250])\n"
     ]
    }
   ],
   "source": [
    "with open('data_train.pickle', 'rb') as handle:\n",
    "    data2 = pickle.load(handle)\n",
    "print(data2.shape)\n",
    "\n",
    "def predictive_score_for_diim_estimation(data_all, train, classname):\n",
    "    score=[]\n",
    "    for d in [1,5,10,20,50,75,100,200,250]:\n",
    "        nan_mask = np.isnan(train) #when calculating the train/test set to \"nan\" all the examples that are for testing so that you do not train on them \n",
    "        test = classname(data_all,train, d)  \n",
    "        pyro.clear_param_store()\n",
    "        test.train_SVI(data_all , ~torch.from_numpy(nan_mask), verbose = False)\n",
    "        test.sample_predict(500)\n",
    "        score.append(test.predictive_score(data_all,  ~torch.from_numpy(nan_mask)))\n",
    "    return score\n",
    "\n",
    "s = predictive_score_for_diim_estimation(data,data2, PMF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = [1,5,10,20,50,75,100,200,250]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(d, s)\n",
    "plt.title(\"Value of the Log Predictive For different #dimensions\")\n",
    "plt.savefig(\"predictive_score_simple.png\")\n",
    "plt.show()\n",
    "pyro.clear_param_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elbo loss: 1154458218.0\n",
      "Elbo loss: 187166798.19140625\n",
      "Elbo loss: 64558026.240234375\n",
      "Elbo loss: 38171677.46191406\n",
      "Elbo loss: 33257379.75\n",
      "Elbo loss: 31130139.90625\n",
      "Elbo loss: 29894891.3359375\n",
      "Elbo loss: 28238318.9453125\n",
      "Elbo loss: 25987876.65625\n",
      "Elbo loss: 23630812.50390625\n",
      "Elbo loss: 22325087.8203125\n",
      "Elbo loss: 21060175.3125\n",
      "Elbo loss: 20078680.4375\n",
      "Elbo loss: 19810330.3359375\n",
      "Elbo loss: 19051402.1875\n",
      "Elbo loss: 18562975.4296875\n",
      "Elbo loss: 18809184.7421875\n",
      "Elbo loss: 18452862.0234375\n",
      "Elbo loss: 18151757.640625\n",
      "Elbo loss: 18007491.515625\n",
      "Elbo loss: 17961212.625\n",
      "Elbo loss: 17562170.2578125\n",
      "Elbo loss: 17172043.828125\n",
      "Elbo loss: 17404447.546875\n",
      "Elbo loss: 17680716.8515625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test = PMF(data, train=data, dim=50)\n",
    "l = test.train_SVI(data, ~torch.from_numpy(nan_mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UA: (10, 1, 1127, 100)\n",
      "VA: (10, 1, 5237, 100)\n",
      "target: (10, 1127, 5237)\n"
     ]
    }
   ],
   "source": [
    "test.sample_predict(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMF MAP training RMSE: 0.36516\n",
      "AUC: 0.83883\n",
      "[[1. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [1. 0. 0. ... 1. 1. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 1. 1. 0.]\n",
      " [1. 0. 0. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[ 1  0  0 ...  1  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 1  0  0 ...  1  8  0]\n",
      " ...\n",
      " [ 8  0  0 ... 10 12  0]\n",
      " [ 1  0  0 ...  4 25  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "test.rmse(data)\n",
    "print(test.get_predictions())\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PMF_intercepts(nn.Module):\n",
    "    # by default our latent space is 50-dimensional\n",
    "    # and we use 400 hidden units\n",
    "    def __init__(self, train, dim):\n",
    "        super().__init__()\n",
    "        \"\"\"Build the Probabilistic Matrix Factorization model using pymc3.\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        self.dim = dim   \n",
    "        self.data = train.copy()\n",
    "        self.n, self.m = self.data.shape\n",
    "        self.map = None\n",
    "        self.bounds = (0,1)\n",
    "        self.losses = None\n",
    "        self.predictions = None\n",
    "\n",
    "\n",
    "        # Perform mean value imputation\n",
    "    \n",
    "        \n",
    "        # Low precision reflects uncertainty; prevents overfitting.\n",
    "        # Set to the mean variance across users and items.\n",
    "        self.alpha_u = (np.mean(self.data, axis=1).mean())**2 / np.std(self.data, axis=1).mean()\n",
    "        self.alpha_v = (np.mean(self.data, axis=0).mean())**2 / np.std(self.data, axis=0).mean()\n",
    "\n",
    "        self.beta_u = (np.mean(self.data, axis=1).mean()) / np.std(self.data, axis=1).mean()\n",
    "        self.beta_v = (np.mean(self.data, axis=0).mean()) / np.std(self.data, axis=0).mean()\n",
    "       \n",
    "        self.bias = self.data.mean()\n",
    "\n",
    "\n",
    "    def model(self, train):\n",
    "\n",
    "        drug_plate = pyro.plate(\"drug_latents\", self.n, dim= -1) #independent users\n",
    "        sideeffect_plate = pyro.plate(\"sideeffect_latents\", self.m, dim= -1) #independent items\n",
    "\n",
    "        with drug_plate: \n",
    "            UA = pyro.sample(\"UA\", dist.Gamma(self.alpha_u, self.beta_u).expand([self.dim]).to_event(1))\n",
    "            drug_intercept = pyro.sample(\"drug_int\", dist.HalfNormal(0.5))\n",
    "            #UA_int = pyro.sample(\"UAint\", dist.Normal(0., 1.))\n",
    "        \n",
    "        with sideeffect_plate:\n",
    "            sideeffect_intercept = pyro.sample(\"sf_int\", dist.HalfNormal(0.5))\n",
    "            VA = pyro.sample(\"VA\", dist.Gamma(self.alpha_v, self.beta_v).expand([self.dim]).to_event(1))\n",
    "            #possibly add intercepts VA_int = pyro.sample(\"VA\", dist.Normal(0., 1.).to_event(1))\n",
    "       \n",
    "        u2_plate = pyro.plate(\"u2_plate\", self.n, dim=-2)\n",
    "       #with u2_plate:\n",
    "           \n",
    "            \n",
    "        with sideeffect_plate, u2_plate: \n",
    "            #Y = pyro.sample(\"target\", dist.Poisson(UA@VA.T  +sideeffect_intercept.T), obs=train ) z[:, np.newaxis] + x\n",
    "            Y = pyro.sample(\"target\", dist.Poisson(UA@VA.T  + (drug_intercept[:, np.newaxis] + sideeffect_intercept.T)), obs=train ) \n",
    "            return Y\n",
    "        \n",
    "\n",
    "    def guide(self, train=None, mask=None):\n",
    "\n",
    "        d_alpha = pyro.param('d_alpha', torch.ones(self.n,self.dim), constraint=constraints.positive)#*self.user_mean)\n",
    "        d_beta = pyro.param('d_beta', 0.5*torch.ones(self.n,self.dim), constraint=constraints.positive)\n",
    "        \n",
    "       # int_mean = pyro.param('int_mean', torch.tensor(1.)*self.user_mean)\n",
    "       # mov_cov = pyro.param('mov_cov', torch.tensor(1.)*0.1,\n",
    "          #                  constraint=constraints.positive)\n",
    "\n",
    "        s_alpha = pyro.param('s_alpha', torch.ones(self.m,self.dim), constraint=constraints.positive)#*self.item_mean)\n",
    "        s_beta = pyro.param('s_beta', 0.5*torch.ones(self.m,self.dim), constraint=constraints.positive)\n",
    "        drug_plate = pyro.plate(\"drug_latents\", self.n, dim= -1) #independent users\n",
    "        sideeffect_plate = pyro.plate(\"sideeffect_latents\", self.m, dim= -1) #independent items\n",
    "        se_t = pyro.param(\"sef_int\", 0.25*torch.ones(self.m), constraint=constraints.positive)\n",
    "        drug_t = pyro.param(\"drug_int_p\", 0.25*torch.ones(self.n), constraint=constraints.positive)\n",
    "        u2_plate = pyro.plate(\"u2_plate\", self.n, dim=-2)\n",
    "\n",
    "       # with u2_plate:\n",
    "          #  drug_intercept = pyro.sample(\"drug_int\", dist.HalfNormal(drug_t).to_event(1))\n",
    "\n",
    "        with drug_plate: \n",
    "            UA = pyro.sample(\"UA\", dist.Gamma(d_alpha, d_beta).to_event(1))\n",
    "            UA_int = pyro.sample(\"UAint\", dist.HalfNormal(drug_t))\n",
    "        with sideeffect_plate: \n",
    "            VA = pyro.sample(\"VA\", dist.Gamma(s_alpha, s_beta).to_event(1))\n",
    "            sideeffect_intercept =  pyro.sample(\"sf_int\", dist.HalfNormal(se_t))\n",
    "    \n",
    "    def train_SVI(self,train, nsteps=250, lr = 0.05, lrd = 1):\n",
    "        logging.basicConfig(format='%(message)s', level=logging.INFO)\n",
    "        svi = SVI(self.model,\n",
    "        self.guide,\n",
    "        optim.ClippedAdam({\"lr\": lr, \"lrd\": lrd}),\n",
    "        loss=Trace_ELBO())\n",
    "        losses = []\n",
    "        for step in range(nsteps):\n",
    "            elbo = svi.step(torch.from_numpy(train).float())\n",
    "            losses.append(elbo)\n",
    "            if step % 10 == 0:\n",
    "                print(\"Elbo loss: {}\".format(elbo))\n",
    "        self.losses = losses\n",
    "        #constrained_params = list(pyro.get_param_store().values())\n",
    "        #PARAMS = [p.unconstrained() for p in constrained_params]\n",
    "        #print(PARAMS)\n",
    "        return losses\n",
    "    \n",
    "    def sample_predict(self, nsamples=500 , verbose=True):\n",
    "        predictive_svi = Predictive(self.model, guide=self.guide, num_samples=nsamples)( None)\n",
    "        if (verbose):\n",
    "            for k, v in predictive_svi.items():\n",
    "                print(f\"{k}: {tuple(v.shape)}\")\n",
    "        table = predictive_svi[\"target\"].numpy()\n",
    "        mc_table = table.mean(axis = 0)\n",
    "        mc_table_std = table.std(axis = 0)\n",
    "        mc_table[mc_table < self.bounds[1]] = self.bounds[0]\n",
    "        mc_table[mc_table >= self.bounds[1]] = self.bounds[1]\n",
    "        self.predictions = mc_table\n",
    "        \n",
    "    \n",
    "    def rmse(self,test):\n",
    "        low, high = self.bounds\n",
    "        test_data = test.copy()\n",
    "        test_data[test_data < high] = low\n",
    "        test_data[test_data >= high] = high\n",
    "        sqerror = abs(test_data - self.predictions) ** 2  # squared error array\n",
    "        mse = sqerror.sum()/(test_data.shape[0]*test_data.shape[1])\n",
    "        print(\"PMF MAP training RMSE: %.5f\" % np.sqrt(mse))\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(test_data.astype(int).flatten(),  self.predictions.astype(int).flatten(), pos_label=1)\n",
    "        metrics.auc(fpr, tpr)\n",
    "        print(\"AUC: %.5f\" % metrics.auc(fpr, tpr))\n",
    "        return np.sqrt(mse) , metrics.auc(fpr, tpr)\n",
    "\n",
    "    def get_predictions(self):\n",
    "        return self.predictions\n",
    "\n",
    "    \n",
    "   \n",
    "test = PMF_intercepts(data, 100)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elbo loss: 16507036.471118927\n",
      "Elbo loss: 16882178.70968628\n",
      "Elbo loss: 16316151.067085266\n",
      "Elbo loss: 16288115.850841522\n",
      "Elbo loss: 16257655.711566925\n",
      "Elbo loss: 16270344.331485748\n",
      "Elbo loss: 16342345.294166565\n",
      "Elbo loss: 15919929.547546387\n",
      "Elbo loss: 16114426.330440521\n",
      "Elbo loss: 15449073.024482727\n",
      "Elbo loss: 15926813.614624977\n",
      "Elbo loss: 15586750.751041412\n",
      "Elbo loss: 15827920.701267242\n",
      "Elbo loss: 15714445.02986908\n",
      "Elbo loss: 15497542.100240707\n",
      "Elbo loss: 15426100.48765564\n",
      "Elbo loss: 15437820.758758545\n",
      "Elbo loss: 15585684.149702072\n",
      "Elbo loss: 15870064.738471985\n",
      "Elbo loss: 15594255.186466217\n",
      "Elbo loss: 15578450.936626434\n",
      "Elbo loss: 15589500.542766571\n",
      "Elbo loss: 15133441.503730774\n",
      "Elbo loss: 15588512.186912537\n",
      "Elbo loss: 15487589.70847702\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[16507036.471118927,\n",
       " 16496506.882324219,\n",
       " 16725043.243679047,\n",
       " 16757541.569824219,\n",
       " 16918150.519577026,\n",
       " 16833062.85477066,\n",
       " 16704408.156257153,\n",
       " 16910661.563728333,\n",
       " 16673162.07164669,\n",
       " 17017526.848858833,\n",
       " 16882178.70968628,\n",
       " 16528738.827140808,\n",
       " 16472368.863552094,\n",
       " 16625404.072410583,\n",
       " 16564866.971096039,\n",
       " 16570515.512786865,\n",
       " 16734607.416477203,\n",
       " 16479054.055652618,\n",
       " 16483060.28685379,\n",
       " 16532277.746879578,\n",
       " 16316151.067085266,\n",
       " 16505781.601371765,\n",
       " 16345559.011214256,\n",
       " 16548068.182533264,\n",
       " 16689426.65473175,\n",
       " 16392662.219470024,\n",
       " 16446702.082932472,\n",
       " 16294120.535728455,\n",
       " 16434014.2137146,\n",
       " 16128583.033908844,\n",
       " 16288115.850841522,\n",
       " 16416165.300584793,\n",
       " 16276428.832092285,\n",
       " 16078770.043344498,\n",
       " 16182646.042268753,\n",
       " 16126891.797328949,\n",
       " 16419918.408088684,\n",
       " 16246004.408210754,\n",
       " 16403620.063171387,\n",
       " 16164927.168022156,\n",
       " 16257655.711566925,\n",
       " 16272669.908325195,\n",
       " 15973345.710792542,\n",
       " 16108965.42982483,\n",
       " 15875432.667793274,\n",
       " 16125924.253845215,\n",
       " 16135030.776966095,\n",
       " 16107974.334237576,\n",
       " 15942221.178153992,\n",
       " 16355228.12330246,\n",
       " 16270344.331485748,\n",
       " 15908247.247271776,\n",
       " 16566857.564457417,\n",
       " 16035639.58191681,\n",
       " 16165265.658138275,\n",
       " 16022235.24751091,\n",
       " 16389654.05859375,\n",
       " 15870910.638858795,\n",
       " 15826703.494167328,\n",
       " 15789247.992897034,\n",
       " 16342345.294166565,\n",
       " 15800009.864595413,\n",
       " 15916690.908027649,\n",
       " 15872281.477426529,\n",
       " 15887774.846412659,\n",
       " 15806945.83843565,\n",
       " 15900864.505126953,\n",
       " 15922861.198984146,\n",
       " 16427988.150596619,\n",
       " 16040998.448371887,\n",
       " 15919929.547546387,\n",
       " 16145212.856880188,\n",
       " 16113568.845108032,\n",
       " 15965631.544433594,\n",
       " 16036502.577526093,\n",
       " 15876524.033576965,\n",
       " 16199060.250073433,\n",
       " 15787710.435646057,\n",
       " 16215290.012413025,\n",
       " 15845512.885177612,\n",
       " 16114426.330440521,\n",
       " 15851728.461212158,\n",
       " 15852956.857584,\n",
       " 15981298.006420135,\n",
       " 15996588.736534119,\n",
       " 15904523.166484833,\n",
       " 15854740.314037323,\n",
       " 15565596.181214333,\n",
       " 15863747.382282257,\n",
       " 15897624.66474533,\n",
       " 15449073.024482727,\n",
       " 15745224.747955322,\n",
       " 15962121.989881516,\n",
       " 16123299.185533524,\n",
       " 15954523.1722641,\n",
       " 16083808.665985107,\n",
       " 15695824.988613129,\n",
       " 15804538.444839478,\n",
       " 15774681.198669434,\n",
       " 15623505.47277069,\n",
       " 15926813.614624977,\n",
       " 15794459.025575638,\n",
       " 16039189.990596771,\n",
       " 15812220.6447258,\n",
       " 15979347.09092331,\n",
       " 15892139.569551468,\n",
       " 15909107.23928833,\n",
       " 15697212.683143616,\n",
       " 15696747.020568848,\n",
       " 16093784.409301758,\n",
       " 15586750.751041412,\n",
       " 15922860.289211273,\n",
       " 15703673.150619507,\n",
       " 15758596.512954712,\n",
       " 15525268.142921448,\n",
       " 15752755.873752594,\n",
       " 15533891.340459824,\n",
       " 15786051.196418762,\n",
       " 15790839.431808472,\n",
       " 15473518.845149994,\n",
       " 15827920.701267242,\n",
       " 15692087.269408703,\n",
       " 15881916.950001717,\n",
       " 15578236.27797699,\n",
       " 15612678.655403137,\n",
       " 15757467.661521912,\n",
       " 15756821.63548088,\n",
       " 15699529.417663574,\n",
       " 15507061.45148468,\n",
       " 15521864.8468256,\n",
       " 15714445.02986908,\n",
       " 15548554.384456635,\n",
       " 15918831.631278992,\n",
       " 15942966.532112122,\n",
       " 15598392.578323364,\n",
       " 15556424.189470291,\n",
       " 15911246.185302734,\n",
       " 15623231.820497513,\n",
       " 15832231.672298431,\n",
       " 15616468.227279663,\n",
       " 15497542.100240707,\n",
       " 15861037.333515167,\n",
       " 15814826.630729675,\n",
       " 15595810.358024597,\n",
       " 15582183.778865814,\n",
       " 15699929.984638214,\n",
       " 15423451.582389832,\n",
       " 15659506.238845825,\n",
       " 15535013.584088326,\n",
       " 15447609.678474426,\n",
       " 15426100.48765564,\n",
       " 15467400.940856934,\n",
       " 15614213.434635162,\n",
       " 15663159.639255524,\n",
       " 15981826.220062256,\n",
       " 15732062.40726471,\n",
       " 15620178.823253632,\n",
       " 15395488.611772537,\n",
       " 15407874.66674614,\n",
       " 15778531.539405823,\n",
       " 15437820.758758545,\n",
       " 15606144.192764282,\n",
       " 15375635.969688416,\n",
       " 15850202.993946075,\n",
       " 15715283.519145966,\n",
       " 15773250.584777832,\n",
       " 15547774.443572998,\n",
       " 15695929.937355042,\n",
       " 15658804.61959076,\n",
       " 15573128.764421463,\n",
       " 15585684.149702072,\n",
       " 16056379.764404297,\n",
       " 15644116.437412262,\n",
       " 15516559.358013153,\n",
       " 15284904.736356735,\n",
       " 15433077.89087677,\n",
       " 15359215.45103836,\n",
       " 15633816.929893494,\n",
       " 15360519.703567505,\n",
       " 15445539.27798462,\n",
       " 15870064.738471985,\n",
       " 15761697.168543339,\n",
       " 15535525.4204216,\n",
       " 15457702.276380539,\n",
       " 15591632.004104614,\n",
       " 15670202.07440567,\n",
       " 15707230.566642761,\n",
       " 15498551.793619156,\n",
       " 15762793.374222755,\n",
       " 15396198.079177856,\n",
       " 15594255.186466217,\n",
       " 15614017.100387573,\n",
       " 15702858.405136585,\n",
       " 15713553.678173065,\n",
       " 15470010.355819702,\n",
       " 15125391.993322372,\n",
       " 15433078.926242828,\n",
       " 15578104.041683197,\n",
       " 15420454.572498322,\n",
       " 15339997.888780117,\n",
       " 15578450.936626434,\n",
       " 15464310.712429047,\n",
       " 15517398.592485428,\n",
       " 15298581.824092865,\n",
       " 15571824.434230804,\n",
       " 15681906.826662064,\n",
       " 15595059.062042236,\n",
       " 15396553.647285461,\n",
       " 15413994.538638115,\n",
       " 15399544.866596222,\n",
       " 15589500.542766571,\n",
       " 15840623.638126373,\n",
       " 15654409.728012085,\n",
       " 15461296.721313477,\n",
       " 15666421.033527374,\n",
       " 15326031.262268066,\n",
       " 15647912.535308838,\n",
       " 15663742.470977783,\n",
       " 15282905.481372833,\n",
       " 15578183.808006287,\n",
       " 15133441.503730774,\n",
       " 15301624.379867554,\n",
       " 15385850.502857208,\n",
       " 15464946.551847458,\n",
       " 15722826.657499313,\n",
       " 15020539.123921394,\n",
       " 15481205.169636726,\n",
       " 15546262.129173279,\n",
       " 15330268.149253845,\n",
       " 15959773.814346313,\n",
       " 15588512.186912537,\n",
       " 15587294.668518066,\n",
       " 15615664.430522919,\n",
       " 15302275.819786072,\n",
       " 15512717.07917881,\n",
       " 15693877.348651886,\n",
       " 15350004.242898941,\n",
       " 15332900.496261597,\n",
       " 15412289.184181213,\n",
       " 15511709.488265991,\n",
       " 15487589.70847702,\n",
       " 15432956.576099396,\n",
       " 15472220.919204712,\n",
       " 15413297.756988525,\n",
       " 15141371.957845688,\n",
       " 15430063.763504028,\n",
       " 15226800.089057922,\n",
       " 15457198.47555542,\n",
       " 15777150.465236664,\n",
       " 15851376.319734573]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test.train_SVI(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UA: (1000, 1, 1127, 100)\n",
      "drug_int: (1000, 1, 1127)\n",
      "sf_int: (1000, 1, 5237)\n",
      "VA: (1000, 1, 5237, 100)\n",
      "target: (1000, 1127, 5237)\n"
     ]
    }
   ],
   "source": [
    "test.sample_predict(1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMF MAP training RMSE: 0.36189\n",
      "AUC: 0.84249\n",
      "[[1. 1. 0. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 1. 1. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 1. 1. 0.]\n",
      " [1. 0. 0. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[ 1  0  0 ...  1  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 1  0  0 ...  1  8  0]\n",
      " ...\n",
      " [ 8  0  0 ... 10 12  0]\n",
      " [ 1  0  0 ...  4 25  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "test.rmse(data)\n",
    "print(test.get_predictions())\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing!!!\n",
    "\n",
    "n,m = data.shape\n",
    "dim=10\n",
    "alpha_u = 4\n",
    "alpha_v=5\n",
    "beta_u=1\n",
    "beta_v=1\n",
    "def model():\n",
    "\n",
    "        drug_plate = pyro.plate(\"drug_latents\", n, dim= -1) #independent users\n",
    "        sideeffect_plate = pyro.plate(\"sideeffect_latents\", m, dim= -1) #independent items\n",
    "   \n",
    "\n",
    "        with drug_plate:\n",
    "            \n",
    "            UA = pyro.sample(\"UA2\", dist.Gamma(alpha_u, beta_u).expand([dim]).to_event(1))\n",
    "            #UA_int = pyro.sample(\"UAint\", dist.Normal(0., 1.))\n",
    "        \n",
    "        with sideeffect_plate:\n",
    "            sideeffect_intercept = pyro.sample(\"sf_int\", dist.HalfNormal(0.5))\n",
    "\n",
    "            VA = pyro.sample(\"VA2\", dist.Gamma(alpha_v, beta_v).expand([dim]).to_event(1))\n",
    "        \n",
    "        u2_plate = pyro.plate(\"u2_plate\", n, dim=-2)\n",
    "        with u2_plate:\n",
    "            drug_intercept = pyro.sample(\"drug_int\", dist.HalfNormal(0.5))\n",
    "\n",
    "        with sideeffect_plate, u2_plate: \n",
    "            Y = pyro.sample(\"target\", dist.Poisson( UA@VA.T+sideeffect_intercept.T ) ) \n",
    "            return Y\n",
    "\n",
    "def guide9():\n",
    "\n",
    "        d_alpha = pyro.param('d_alpha', 5*torch.ones(n,dim), constraint=constraints.positive)#*self.user_mean)\n",
    "        d_beta = pyro.param('d_beta', 0.05*torch.ones(n,dim), constraint=constraints.positive)\n",
    "        # int_mean = pyro.param('int_mean', torch.tensor(1.)*self.user_mean)\n",
    "        # mov_cov = pyro.param('mov_cov', torch.tensor(1.)*0.1,\n",
    "            #                  constraint=constraints.positive)\n",
    "        s_alpha = pyro.param('s_alpha', 5*torch.ones(m,dim), constraint=constraints.positive)#*self.item_mean)\n",
    "        s_beta = pyro.param('s_beta', 0.05*torch.ones(m,dim), constraint=constraints.positive)\n",
    "        drug_plate = pyro.plate(\"drug_latents\",n, dim= -1) #independent users\n",
    "        sideeffect_plate = pyro.plate(\"sideeffect_latents\",m, dim= -1) #independent items\n",
    "        se_t = pyro.param(\"sef_int\", 0.25*torch.ones(m), constraint=constraints.positive)\n",
    "        drug_t = pyro.param(\"drug_int_p\", 0.25*torch.ones(n), constraint=constraints.positive)\n",
    "        u2_plate = pyro.plate(\"u2_plate\", n, dim=-2)\n",
    "        with u2_plate:\n",
    "            drug_intercept = pyro.sample(\"drug_int\", dist.HalfNormal(drug_t).to_event(1))\n",
    "\n",
    "        \n",
    "        with drug_plate: \n",
    "            UA = pyro.sample(\"UA2\", dist.Gamma(d_alpha, d_beta).to_event(1))\n",
    "            # UA_int = pyro.sample(\"UAint\", dist.Normal(int_mean, mov_cov).to_event(1))\n",
    "     \n",
    "        \n",
    "        with sideeffect_plate: \n",
    "            VA = pyro.sample(\"VA2\", dist.Gamma(s_alpha, s_beta).to_event(1))\n",
    "            sideeffect_intercept =  pyro.sample(\"sf_int\", dist.HalfNormal(se_t))\n",
    "\n",
    "    \n",
    "def model6():\n",
    "    d = 50\n",
    "    N = 100\n",
    "    \n",
    "    # Vector of variances for each of the d variables\n",
    "    theta = pyro.sample(\"theta\", dist.HalfCauchy(torch.ones(d)))\n",
    "    # Lower cholesky factor of a correlation matrix\n",
    "    concentration = torch.ones(\n",
    "        (), \n",
    "    )  # Implies a uniform distribution over correlation matrices\n",
    "    L_omega = pyro.sample(\"L_omega\", dist.LKJCholesky(d, concentration))\n",
    "    # Lower cholesky factor of the covariance matrix\n",
    "    L_Omega = torch.mm(torch.diag(theta.sqrt()), L_omega)\n",
    "    # For inference with SVI, one might prefer to use torch.bmm(theta.sqrt().diag_embed(), L_omega)\n",
    "\n",
    "    # Vector of expectations\n",
    "    mu = torch.zeros(d)\n",
    "\n",
    "    with pyro.plate(\"observations\", N):\n",
    "        obs = pyro.sample(\"obs\", dist.MultivariateNormal(mu, scale_tril=L_Omega))\n",
    "    return obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Trace Shapes:            \n",
      "     Param Sites:            \n",
      "    Sample Sites:            \n",
      "       theta dist  50 |      \n",
      "            value  50 |      \n",
      "         log_prob  50 |      \n",
      "     L_omega dist     | 50 50\n",
      "            value     | 50 50\n",
      "         log_prob     |      \n",
      "observations dist     |      \n",
      "            value 100 |      \n",
      "         log_prob     |      \n",
      "         obs dist 100 | 50   \n",
      "            value 100 | 50   \n",
      "         log_prob 100 |      \n",
      "          Trace Shapes:                    \n",
      "           Param Sites:                    \n",
      "                d_alpha      1127   10     \n",
      "                 d_beta      1127   10     \n",
      "                s_alpha      5237   10     \n",
      "                 s_beta      5237   10     \n",
      "                sef_int           5237     \n",
      "             drug_int_p           1127     \n",
      "          Sample Sites:                    \n",
      "      drug_latents dist              |     \n",
      "                  value      1127    |     \n",
      "               log_prob              |     \n",
      "sideeffect_latents dist              |     \n",
      "                  value      5237    |     \n",
      "               log_prob              |     \n",
      "          u2_plate dist              |     \n",
      "                  value      1127    |     \n",
      "               log_prob              |     \n",
      "          drug_int dist 1127    1    | 1127\n",
      "                  value 1127    1    | 1127\n",
      "               log_prob 1127    1    |     \n",
      "               UA2 dist      1127    |   10\n",
      "                  value      1127    |   10\n",
      "               log_prob      1127    |     \n",
      "               VA2 dist      5237    |   10\n",
      "                  value      5237    |   10\n",
      "               log_prob      5237    |     \n",
      "            sf_int dist      5237    |     \n",
      "                  value      5237    |     \n",
      "               log_prob      5237    |     \n"
     ]
    }
   ],
   "source": [
    "trace=poutine.trace(model6).get_trace()\n",
    "trace.compute_log_prob()\n",
    "print(trace.format_shapes())\n",
    "\n",
    "trace=poutine.trace(guide9).get_trace()\n",
    "trace.compute_log_prob()\n",
    "print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1127, 5237])\n",
      "torch.Size([1127, 5237])\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
